{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e571f3a0-c096-4ae9-aa2b-129c896a602b",
   "metadata": {},
   "source": [
    "# CH03 LLM,RAG 개인 필수 과제"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b44fd0-0f5b-481d-9551-0966e20f6cba",
   "metadata": {},
   "source": [
    "### 01_API key_load_환경변수(environment_variables)사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2970b43a-7ba1-48e6-8b0f-58f5b0c02ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes key\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# .env 파일을 불러오기\n",
    "load_dotenv(r\"C:\\Users\\241011\\Documents\\key.env\")\n",
    "\n",
    "# 환경 변수 중 하나를 확인 (예: 'KEY'라는 환경 변수 확인)\n",
    "key = os.getenv(\"OPENAI_API_KEY\")  # 여기서 \"KEY\"는 .env 파일에 정의된 환경 변수의 이름입니다.\n",
    "\n",
    "# 환경 변수가 존재하는지 확인\n",
    "if key:  # key 값이 존재하면 True, 존재하지 않으면 False\n",
    "    print(\"yes key\")\n",
    "else:\n",
    "    print(\"no key\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45104ad9-5f5c-4d6e-a03a-2ae5509dfce0",
   "metadata": {},
   "source": [
    "### 02_PDF loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b12d0b3-bedb-4077-819c-814c71c7db2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "▹ 미국 백악관 예산관리국, 정부의 책임 있는 AI 조달을 위한 지침 발\n",
      "\n",
      "위원회에 따르면 연방정부와 법 집행기관에서 얼굴인식 기술이 빠르게 도입되\n",
      "대통령의 AI 행정명령에 따라 연방정부의 책임 있는 AI 조달을  지원하\n",
      "르면 AI는 고급 데이터 분석, 디지털 증거 수집, 이미지와 비디오  분\n",
      " AI 도입 모범사례와 거버넌스 프레임워크,  정책 옵션을 토대로 공공 \n",
      "입안자를 대상으로 생성AI의 공익적 활용과 경제사회적  균형 달성, 위험\n",
      " 2024년 3분기 AI 스타트업은 전체 벤처 투자의 31%를 유치했으며\n",
      "동영상 제작, 동영상 편집, 오디오 생성과 같은 기능을 지원하는  ‘메타\n",
      "두 처리할 수 있는 모델과 모바일 기기에서 실행 가능한 경량  모델을 포\n",
      "모’ 공개 n 앨런AI연구소가 공개한 멀티모달 LLM 제품군 몰모는 벤치\n",
      "스로 사용할 수 있는 경량 모델 ‘레 미니스트로’를  미스트랄 3B와 미\n",
      " 맥락 속에서 주요 정보를 기억해 이용자에게 최적화된 답변을 제공하는 A\n",
      " 바탕으로 인공 신경망을 이용한 머신러닝의 토대가 되는  방법을 개발한 \n",
      "국 국무부는 바이든 대통령의 AI 행정명령에 따라 국제협력을 통해 포괄적\n",
      "공자가 안전성 평가에 참조할 수 있는 ‘AI 안전성에 대한  평가 관점 \n",
      "마인드가 강화학습 방식으로 반도체 칩 레이아웃을 설계하여 사람이 몇 주에\n",
      "1의 오리 고센 CEO는 AI 모델 개발에 주로 활용되는 트랜스포머  아\n",
      "터가 설문조사를 통해 근로자 관점의 자동화 기술의 영향을 조사한 결과, \n",
      " 2025년 이직을 계획 중이며, 58%는 2024년 중 현재보다  더 \n",
      "따르면 생성AI의 도입으로 중장기적으로 소프트웨어 엔지니어링에서 데이터 \n",
      "에 대한 생성AI의 수행 능력을 분석해 인간을 대체할  가능성을 평가한 \n",
      "리 법인으로 매년 학제간  학술대회(NeurIPS)를 주최 - 이번 제3\n",
      " Seongnam-si, Gyeonggi-do, Republic of K\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import fitz  # PyMuPDF\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.schema import Document\n",
    "from typing import List\n",
    "\n",
    "# PDF 파일 로드 (PyMuPDF 사용)\n",
    "def extract_text_from_pdf(pdf_path: str) -> List[str]:\n",
    "    \"\"\"PyMuPDF를 사용하여 PDF에서 각 페이지의 텍스트를 추출\"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text_pages = []\n",
    "    \n",
    "    # 각 페이지에서 텍스트 추출\n",
    "    for page_num in range(doc.page_count):\n",
    "        page = doc.load_page(page_num)\n",
    "        text = page.get_text(\"text\")\n",
    "        text_pages.append(text)\n",
    "    \n",
    "    return text_pages\n",
    "\n",
    "# 줄바꿈 제거 함수\n",
    "def remove_newlines_except_after_period(text: str) -> str:\n",
    "    \"\"\"마침표 다음의 줄바꿈을 제외한 모든 줄바꿈을 제거\"\"\"\n",
    "    return re.sub(r'(?<!\\.)(\\n|\\r\\n)', ' ', text)\n",
    "\n",
    "# 연속된 '·' 문자와 숫자 뒤 텍스트 제거하는 함수\n",
    "def remove_page_numbers_and_bullet_points(text: str) -> str:\n",
    "    \"\"\"페이지 번호와 구분 기호 ···· 등 제거\"\"\"\n",
    "    # 숫자, 특수문자(·) 뒤에 이어지는 텍스트를 제거\n",
    "    text = re.sub(r'\\d+\\s*▹\\s*[\\u25CB\\u2022\\u2023]*\\s*\\d+', '', text)  # 페이지 번호나 번호 뒤 '·' 제거\n",
    "    text = re.sub(r'·+', '', text)  # 연속된 '·' 제거\n",
    "    return text\n",
    "\n",
    "# 페이지 처리 함수\n",
    "def process_pdf(pdf_path: str) -> List[Document]:\n",
    "    pages_text = extract_text_from_pdf(pdf_path)  # PDF에서 텍스트 추출\n",
    "    \n",
    "    processed_documents = []\n",
    "    \n",
    "    for page_text in pages_text:\n",
    "        # 텍스트에서 불필요한 줄바꿈 제거\n",
    "        cleaned_text = remove_newlines_except_after_period(page_text)\n",
    "        \n",
    "        # 페이지 번호와 구분 기호 제거\n",
    "        cleaned_text = remove_page_numbers_and_bullet_points(cleaned_text)\n",
    "        \n",
    "        # 처리된 페이지 정보를 새로운 Document로 저장\n",
    "        processed_documents.append(Document(page_content=cleaned_text, metadata={}))\n",
    "    \n",
    "    return processed_documents\n",
    "\n",
    "# PDF 파일 처리\n",
    "pdf_path = \"00_data/인공지능산업최신동향_2024년11월호.pdf\"\n",
    "pages = process_pdf(pdf_path)\n",
    "\n",
    "# 결과 출력 (예시)\n",
    "for doc in pages:\n",
    "    print(doc.page_content[90:130])  # 처리된 텍스트 일부 출력\n",
    "\n",
    "#print(f'type : {type(pages)} / len : {len(pages)} / pages : {pages}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7bfebb-ff81-4179-b422-1af49b515300",
   "metadata": {},
   "source": [
    "### 03_TextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e9b3fde-d7ae-406a-9860-022961067c09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_size:\n",
      "[10, 1084, 17, 1542, 1567, 1547, 1523, 1568, 1488, 1451, 1673, 1378, 1351, 1460, 1727, 1587, 1594, 1406, 1620, 1430]\n",
      "====================================================================================================\n",
      "Chunk 1: page_content='2024년 11월호'\n",
      "Chunk 2: page_content='2024년 11월호 Ⅰ. 인공지능 산업 동향 브리프  1. 정책/법제     ▹ 미국 민권위원회, 연방정부의 얼굴인식 기술 사용에 따른 민권 영향 분석 1    ▹ 미국 백악관 예산관리국, 정부의 책임 있는 AI 조달을 위한 지침 발표 2    ▹ 유로폴, 법 집행에서 AI의 이점과 과제를 다룬 보고서 발간 3    ▹ OECD, 공공 부문의 AI 도입을 위한 G7 툴킷 발표 4    ▹ 세계경제포럼, 생성AI 시대의 거버넌스 프레임워크 제시 5    2. 기업/산업     ▹ CB인사이츠 분석 결과, 2024년 3분기 벤처 투자 31%가 AI 스타트업에 집중 6    ▹ 메타, 동영상 생성AI 도구 ‘메타 무비 젠’ 공개 7    ▹ 메타, 이미지와 텍스트 처리하는 첫 멀티모달 AI 모델 ‘라마 3.2’ 공개 8    ▹ 앨런AI연구소, 벤치마크 평가에서 GPT-4o 능가하는 성능의 오픈소스 LLM ‘몰모’ 공개 9    ▹ 미스트랄AI, 온디바이스용 AI 모델 ‘레 미니스트로’ 공개 10    ▹ 카카오, 통합 AI 브랜드 겸 신규 AI 서비스 ‘카나나’ 공개 11  3. 기술/연구    ▹ 2024년 노벨 물리학상과 화학상, AI 관련 연구자들이 수상 12    ▹ 미국 국무부, AI 연구에서 국제협력을 위한 ‘글로벌 AI 연구 의제’ 발표 13    ▹ 일본 AI안전연구소, AI 안전성에 대한 평가 관점 가이드 발간 14    ▹ 구글 딥마인드, 반도체 칩 레이아웃 설계하는 AI 모델 ‘알파칩’ 발표 15    ▹ AI21 CEO, AI 에이전트에 트랜스포머 아키텍처의 대안 필요성 강조 16      4. 인력/교육         ▹ MIT 산업성과센터, 근로자 관점에서 자동화 기술의 영향 조사 17    ▹ 다이스 조사, AI 전문가의 73%는 2025년 중 이직 고려 18    ▹ 가트너 예측, AI로 인해 엔지니어링 인력의 80%가 역량 향상 필요  19    ▹ 인디드 조사 결과, 생성AI가 인간 근로자 대체할 가능성은 희박 20      Ⅱ. 주요 행사   ▹NeurIPS 2024  21   ▹GenAI Summit Maroc 2024  21   ▹AI Summit Seoul 2024  21'\n",
      "Chunk 3: page_content='Ⅰ. 인공지능 산업 동향 브리프'\n",
      "Chunk 4: page_content='1. 정책/법제   2. 기업/산업  3. 기술/연구   4. 인력/교육 1 미국 민권위원회, 연방정부의 얼굴인식 기술 사용에 따른 민권 영향 분석 n 미국 민권위원회에 따르면 연방정부와 법 집행기관에서 얼굴인식 기술이 빠르게 도입되고  있으나 이를 관리할 지침과 감독의 부재로 민권 문제를 초래할 위험 존재 n 미국 민권위원회는 연방정부의 책임 있는 얼굴인식 기술 사용을 위해 운영 프로토콜 개발과  실제 사용 상황의 얼굴인식 기술 평가 및 불평등 완화, 지역사회의 의견 수렴 등을 권고 KEY Contents £ 연방정부의 얼굴인식 기술 도입에 대한 지침과 감독 부재로 민권 문제를 초래할 위험 존재 n 미국 민권위원회(U.S. Commission on Civil Rights)가 2024년 9월 19일 연방정부의 얼굴인식  기술 사용이 민권에 미치는 영향을 분석한 보고서를 발간 ∙AI 기술의 일종인 얼굴인식 기술은 연방정부와 법 집행기관에서 빠르게 도입되고 있으며, 일례로  법무부 연방수사국(FBI)은 범죄 수사 및 용의자 수색용 단서 확보를 위해 얼굴인식 기술을 가장 빈번히 사용 ∙그러나 얼굴인식 기술의 책임 있는 사용을 위한 연방 지침과 감독은 실제 활용 사례보다 뒤처졌으며,  현재 연방정부의 얼굴인식 기술이나 여타 AI 기술 사용을 명시적으로 규제하는 법률도 부재   n 보고서에 따르면 얼굴인식 기술의 무분별한 사용은 편향, 개인정보 침해, 적법 절차의 미준수  및 차별적 영향과 같은 민권 문제를 초래할 위험 보유 ∙얼굴인식 기술의 정확도는 인종, 성별, 연령 등 인구통계학적 요인에 따라 달라질 수 있으며, 이는 식별  오류 및 부정확한 체포로 이어져 유색인종을 비롯한 특정 집단에 차별적 결과를 초래할 위험 존재 ∙정부 기관이 사전 영장이나 정당한 이유 없이 얼굴인식 기술을 광범위하게 사용할 경우 개인을  지속적으로 추적하고 감시함으로써 개인정보 보호 권리에 심각한 영향을 미칠 위험 존재 ∙법 집행기관의 얼굴인식 기술 사용 시 부정확한 식별 및 편향으로 인해 개인이 법의 보호를 받아  공정하고 올바르게 대우받을 권리를 침해할 가능성도 존재 £ 민권위원회, 연방정부의 책임 있는 얼굴인식 기술 사용을 위한 권고사항 제시 n 민권위원회는 연방정부의 얼굴인식 기술 사용과 관련해 다음과 같은 권고사항을 제시 ∙국립표준기술연구소(NIST)는 정부 기관의 얼굴인식 기술 시스템 도입 시의 효과와 공평성, 정확성  평가에 사용할 수 있는 운영 테스트 프로토콜의 개발 필요 ∙각 연방정부 기관의 최고AI책임자는 실제 사용 상황에서 얼굴인식 기술을 평가하고 차별이나 편견으로  인한 불평등을 완화하며, 얼굴인식 기술의 사용으로 영향을 받는 지역사회의 의견을 수렴 필요 ∙얼굴인식 기술 제공업체는 다양한 인구통계 집단에 대한 높은 정확도를 보장하기 위해 지속적인 교육과  지원, 업데이트를 제공 필요  ☞ 출처: U.S. Commission on Civil Rights, The Civil Rights Implications of the Federal Use of Facial Recognition Technology, 2024.09.19.'\n",
      "Chunk 5: page_content='SPRi AI Brief |   2024-11월호 2 미국 백악관 예산관리국, 정부의 책임 있는 AI 조달을 위한 지침 발표 n 미국 백악관 예산관리국이 바이든 대통령의 AI 행정명령에 따라 연방정부의 책임 있는 AI 조달을  지원하기 위한 지침을 발표  n 지침은 정부 기관의 AI 조달 시 AI의 위험과 성과를 관리할 수 있는 모범 관행의 수립 및 최상의 AI  솔루션을 사용하기 위한 공급업체 시장의 경쟁 보장, 정부 기관 간 협업을 요구   KEY Contents £ 백악관 예산관리국, 연방정부의 AI 조달 시 책임성을 증진하기 위한 모범 관행 제시 n 미국 백악관 예산관리국(OMB)이 바이든 대통령의 AI 행정명령에 따른 후속 조치로 2024년 10월 3일  ‘정부의 책임 있는 AI 조달 지침(M-24-18)’을 발표 ∙미국 연방정부는 2023년 1,000억 달러 이상의 IT 제품과 서비스를 구매한 미국 경제 최대 규모의 단일  구매자로서 구매력을 활용해 책임 있는 AI의 발전을 뒷받침할 계획 ∙이번 지침은 △AI 위험과 성과 관리 △AI 시장의 경쟁 촉진 △연방정부 전반의 협업 보장이라는 3개  전략적 목표에 대하여 권고사항을 제시 n (AI 위험과 성과 관리) 예산관리국의 지침은 AI 시스템의 구축, 훈련, 배포 방식의 복잡성을 고려해  AI의 위험과 성과를 관리하기 위한 모범 관행을 다음과 같이 제시 ∙정부 기관의 개인정보 보호 담당자가 AI 조달 프로세스에 조기에 지속적으로 참여해 개인정보 보호  위험을 식별 및 관리하고 법률과 정책 준수를 보장 ∙정부 기관과 공급업체와 간 협력으로 AI 솔루션이 조달되는 시기와 해당 조달로 인해 시민 권리와  안전에 영향을 미치는 AI에 대하여 추가로 위험관리가 필요한 시점을 파악 ∙성과 기반의 혁신적 조달 기법을 활용해 정부 기관이 위험을 효과적으로 관리 및 완화하고 성과를 향상할  수 있도록 장려하는 한편, 정부 데이터와 지식재산권을 보호하는 방식으로 계약 조건을 협상  n (AI 시장의 경쟁 촉진) 지침은 정부 기관이 최상의 AI 솔루션을 사용할 수 있도록 공급업체 시장에서  강력한 경쟁을 보장할 것을 요구   ∙계약 요건 수립 시 공급업체 의존성을 최소화할 수 있는 인수 원칙을 적용하고, 시장 조사와 요구사항  개발, 공급업체 평가 절차에서 상호운용성과 투명성을 고려하며, 혁신적 조달 관행을 활용해 우수한  계약업체 성과와 정부 기관의 임무 성과를 보장 n (연방정부 전반의 협업 보장) 빠르게 발전하는 AI 기술환경의 위험관리를 위해 AI 전문지식을 갖춘  공무원과 조달, 개인정보보호, 사이버보안 전문가를 포함하는 협업 팀을 구성해 전략적 조달을 지원   ∙각 정부 기관은 기관 간 협의회를 구성해 효과적이고 책임 있는 AI 조달을 지원하고, 협업 시 기관 목표에  가장 적합한 AI 투자 식별 및 우선순위 지정, AI 배포 역량 개발, AI 모범 활용 사례 채택 증진 등을 고려 ☞ 출처: The White House, FACT SHEET: OMB Issues Guidance to Advance the Responsible Acquisition of AI in  Government, 2024.10.03.'\n"
     ]
    }
   ],
   "source": [
    "# CharacterTextSplitter\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# 텍스트를 두 줄바꿈(\"\\n\\n\")을 기준으로 분할, 최대 500자, 50자 겹침, 정규식 아닌 일반 문자열로 처리\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\\n\",  # 텍스트를 두 줄바꿈 기준으로 분할\n",
    "    chunk_size=500,    # 각 덩어리의 최대 크기 500자\n",
    "    chunk_overlap=50,  # 각 덩어리 간 50자 겹침\n",
    "    length_function=len,  # 텍스트 길이 계산에 `len()` 사용\n",
    "    is_separator_regex=False  # separator는 정규식이 아닌 일반 문자열로 처리\n",
    ")\n",
    "\n",
    "splits_CTS = text_splitter.split_documents(pages)\n",
    "\n",
    "# Document 객체의 page_content 속성에서 텍스트의 길이를 측정\n",
    "char_list = []\n",
    "for i in range(len(splits_CTS)):\n",
    "    char_list.append(len(splits_CTS[i].page_content))\n",
    "print(\"chunk_size:\")\n",
    "print(char_list[:20])\n",
    "print(\"=\"*100)\n",
    "\n",
    "# 상위 50개 청크 선택\n",
    "top_50_chunks_CTS = splits_CTS[:5]  # 순차적으로 상위 50개 청크 선택(제출용 5개)\n",
    "\n",
    "# 상위 50개 청크 출력\n",
    "for i, chunk in enumerate(top_50_chunks_CTS):\n",
    "    print(f\"Chunk {i + 1}: {chunk}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2becd546-eecc-45bf-9229-76e57b270611",
   "metadata": {},
   "source": [
    "#### CharacterTextSplitter(비추천)\n",
    "\n",
    "`CharacterTextSplitter`는 단일 구분자를 기준으로 텍스트를 분리합니다. 주로 하나의 특정 구분자(예: 공백, 구두점 등)를 사용하여 텍스트를 여러 조각으로 나누는 방식입니다. 이 방법은 텍스트가 일정한 패턴이나 규칙을 따를 때 유용하지만, 텍스트의 길이가 너무 길어져서 모델의 최대 토큰 수(맥스 토큰)를 초과할 수 있습니다. 이 경우, 텍스트를 분할할 때 자연스럽지 않거나 부자연스러운 지점에서 끊어질 수 있어, 전체적인 의미나 흐름이 손상될 위험이 존재합니다.\n",
    "\n",
    "따라서, `CharacterTextSplitter`는 맥스 토큰 수를 유지하려는 목적에서는 일부 한계를 가질 수 있으며, 긴 텍스트를 처리할 때는 그 자체로 문제가 발생할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9616a854-9c3a-42eb-9128-e0efe69b6857",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_size:\n",
      "[10, 499, 497, 181, 17, 497, 496, 499, 188, 497, 498, 499, 214, 498, 495, 497, 201, 500, 496, 498]\n",
      "====================================================================================================\n",
      "Chunk 1: page_content='2024년 11월호'\n",
      "Chunk 2: page_content='2024년 11월호 Ⅰ. 인공지능 산업 동향 브리프  1. 정책/법제     ▹ 미국 민권위원회, 연방정부의 얼굴인식 기술 사용에 따른 민권 영향 분석 1    ▹ 미국 백악관 예산관리국, 정부의 책임 있는 AI 조달을 위한 지침 발표 2    ▹ 유로폴, 법 집행에서 AI의 이점과 과제를 다룬 보고서 발간 3    ▹ OECD, 공공 부문의 AI 도입을 위한 G7 툴킷 발표 4    ▹ 세계경제포럼, 생성AI 시대의 거버넌스 프레임워크 제시 5    2. 기업/산업     ▹ CB인사이츠 분석 결과, 2024년 3분기 벤처 투자 31%가 AI 스타트업에 집중 6    ▹ 메타, 동영상 생성AI 도구 ‘메타 무비 젠’ 공개 7    ▹ 메타, 이미지와 텍스트 처리하는 첫 멀티모달 AI 모델 ‘라마 3.2’ 공개 8    ▹ 앨런AI연구소, 벤치마크 평가에서 GPT-4o 능가하는 성능의 오픈소스 LLM ‘몰모’ 공개 9    ▹ 미스트랄AI, 온디바이스용 AI 모델 ‘레 미니스트로’'\n",
      "Chunk 3: page_content='LLM ‘몰모’ 공개 9    ▹ 미스트랄AI, 온디바이스용 AI 모델 ‘레 미니스트로’ 공개 10    ▹ 카카오, 통합 AI 브랜드 겸 신규 AI 서비스 ‘카나나’ 공개 11  3. 기술/연구    ▹ 2024년 노벨 물리학상과 화학상, AI 관련 연구자들이 수상 12    ▹ 미국 국무부, AI 연구에서 국제협력을 위한 ‘글로벌 AI 연구 의제’ 발표 13    ▹ 일본 AI안전연구소, AI 안전성에 대한 평가 관점 가이드 발간 14    ▹ 구글 딥마인드, 반도체 칩 레이아웃 설계하는 AI 모델 ‘알파칩’ 발표 15    ▹ AI21 CEO, AI 에이전트에 트랜스포머 아키텍처의 대안 필요성 강조 16      4. 인력/교육         ▹ MIT 산업성과센터, 근로자 관점에서 자동화 기술의 영향 조사 17    ▹ 다이스 조사, AI 전문가의 73%는 2025년 중 이직 고려 18    ▹ 가트너 예측, AI로 인해 엔지니어링 인력의 80%가 역량 향상 필요  19'\n",
      "Chunk 4: page_content='▹ 가트너 예측, AI로 인해 엔지니어링 인력의 80%가 역량 향상 필요  19    ▹ 인디드 조사 결과, 생성AI가 인간 근로자 대체할 가능성은 희박 20      Ⅱ. 주요 행사   ▹NeurIPS 2024  21   ▹GenAI Summit Maroc 2024  21   ▹AI Summit Seoul 2024  21'\n",
      "Chunk 5: page_content='Ⅰ. 인공지능 산업 동향 브리프'\n"
     ]
    }
   ],
   "source": [
    "# RecursiveCharacterTextSplitter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "recursive_text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "splits_RCTS = recursive_text_splitter.split_documents(pages)\n",
    "\n",
    "# Document 객체의 page_content 속성에서 텍스트의 길이를 측정\n",
    "char_list = []\n",
    "for i in range(len(splits_RCTS)):\n",
    "    char_list.append(len(splits_RCTS[i].page_content))\n",
    "print(\"chunk_size:\")\n",
    "print(char_list[:20])\n",
    "print(\"=\"*100)\n",
    "\n",
    "# 상위 50개 청크 선택\n",
    "top_50_chunks_RCTS = splits_RCTS[:5]  # 순차적으로 상위 50개 청크 선택(제출용 5개)\n",
    "\n",
    "# 상위 50개 청크 출력\n",
    "for i, chunk in enumerate(top_50_chunks_RCTS):\n",
    "    print(f\"Chunk {i + 1}: {chunk}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6aac0cd-914a-4059-aded-cbb14263d1ab",
   "metadata": {},
   "source": [
    "#### RecursiveCharacterTextSplitter(추천)\n",
    "\n",
    "`RecursiveCharacterTextSplitter`는 여러 구분자를 사용하여 텍스트를 재귀적으로 나누는 방법입니다. 이 방식은 하나의 구분자로 텍스트를 나눈 뒤, 그 결과물에서 또 다른 구분자를 사용해 분할을 반복하는 방식입니다. 이 과정을 통해 더 적절한 지점에서 텍스트를 나누어, 최대 토큰 수를 초과하거나 부족한 상황에 맞게 텍스트의 길이를 동적으로 조절할 수 있습니다.\n",
    "\n",
    "주요 특징은 **맥스 토큰**을 초과할 경우 다른 구분자를 찾아서 분할하고, 반대로 토큰 수가 부족할 경우 자연스럽게 텍스트의 길이를 재조정할 수 있다는 점입니다. 이 과정은 구분자를 재귀적으로 탐색하여 길이를 맞추기 때문에 더 유연하고 효율적으로 텍스트를 다룰 수 있습니다.\n",
    "\n",
    "따라서, `RecursiveCharacterTextSplitter`는 텍스트의 의미를 최대한 유지하면서도 효율적으로 토큰 길이를 조정할 수 있어, **대규모 텍스트 처리**에서 훨씬 더 안정적이고 신뢰할 수 있는 분할 방법이 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9644d6b-5de8-46cd-b2be-53779929dc6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Count (for each chunk):\n",
      "[9, 472, 462, 20, 23, 497, 497, 500, 497, 499, 383, 497, 496, 384, 481, 499, 352, 495, 499, 437]\n",
      "====================================================================================================\n",
      "Chunk 1: page_content='2024년 11월호'\n",
      "Chunk 2: page_content='2024년 11월호 Ⅰ. 인공지능 산업 동향 브리프  1. 정책/법제     ▹ 미국 민권위원회, 연방정부의 얼굴인식 기술 사용에 따른 민권 영향 분석 1    ▹ 미국 백악관 예산관리국, 정부의 책임 있는 AI 조달을 위한 지침 발표 2    ▹ 유로폴, 법 집행에서 AI의 이점과 과제를 다룬 보고서 발간 3    ▹ OECD, 공공 부문의 AI 도입을 위한 G7 툴킷 발표 4    ▹ 세계경제포럼, 생성AI 시대의 거버넌스 프레임워크 제시 5    2. 기업/산업     ▹ CB인사이츠 분석 결과, 2024년 3분기 벤처 투자 31%가 AI 스타트업에 집중 6    ▹ 메타, 동영상 생성AI 도구 ‘메타 무비 젠’ 공개 7    ▹ 메타, 이미지와 텍스트 처리하는 첫 멀티모달 AI 모델 ‘라마 3.2’ 공개 8    ▹ 앨런AI연구소, 벤치마크 평가에서 GPT-4o 능가하는 성능의 오픈소스 LLM ‘몰모’ 공개 9    ▹ 미스트랄AI, 온디바이스용 AI 모델 ‘레'\n",
      "Chunk 3: page_content='미니스트로’ 공개 10    ▹ 카카오, 통합 AI 브랜드 겸 신규 AI 서비스 ‘카나나’ 공개 11  3. 기술/연구    ▹ 2024년 노벨 물리학상과 화학상, AI 관련 연구자들이 수상 12    ▹ 미국 국무부, AI 연구에서 국제협력을 위한 ‘글로벌 AI 연구 의제’ 발표 13    ▹ 일본 AI안전연구소, AI 안전성에 대한 평가 관점 가이드 발간 14    ▹ 구글 딥마인드, 반도체 칩 레이아웃 설계하는 AI 모델 ‘알파칩’ 발표 15    ▹ AI21 CEO, AI 에이전트에 트랜스포머 아키텍처의 대안 필요성 강조 16      4. 인력/교육         ▹ MIT 산업성과센터, 근로자 관점에서 자동화 기술의 영향 조사 17    ▹ 다이스 조사, AI 전문가의 73%는 2025년 중 이직 고려 18    ▹ 가트너 예측, AI로 인해 엔지니어링 인력의 80%가 역량 향상 필요  19    ▹ 인디드 조사 결과, 생성AI가 인간 근로자 대체할 가능성은 희박 20      Ⅱ. 주요 행사   ▹NeurIPS 2024  21   ▹GenAI Summit'\n",
      "Chunk 4: page_content='Maroc 2024  21   ▹AI Summit Seoul 2024  21'\n",
      "Chunk 5: page_content='Ⅰ. 인공지능 산업 동향 브리프'\n"
     ]
    }
   ],
   "source": [
    "# tiktoken 라이브러리와 langchain의 RecursiveCharacterTextSplitter 임포트\n",
    "import tiktoken\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 'cl100k_base' 토크나이저를 가져옴 (OpenAI GPT 모델용)\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# tiktoken을 사용하여 텍스트의 토큰 길이를 계산하는 함수 정의\n",
    "def tiktoken_len(text):\n",
    "    # 입력된 텍스트를 'cl100k_base' 토크나이저로 인코딩하여 토큰으로 변환\n",
    "    tokens = tokenizer.encode(text)\n",
    "    # 토큰의 개수를 반환\n",
    "    return len(tokens)\n",
    "\n",
    "# 텍스트 분할기 설정\n",
    "recursive_text_splitter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap = 0, length_function = tiktoken_len)\n",
    "# 페이지를 500자씩 분할하고, 중복되는 텍스트가 없도록 설정\n",
    "splits_RCTS_tiktoken = recursive_text_splitter.split_documents(pages)  # 분할된 텍스트를 docs 변수에 저장\n",
    "\n",
    "# Document 객체의 page_content 속성에서 텍스트의 길이를 측정\n",
    "token_list = []  # 토큰 수를 저장할 리스트\n",
    "for i in range(len(splits_RCTS_tiktoken)):\n",
    "    token_list.append(tiktoken_len(splits_RCTS_tiktoken[i].page_content))  # page_content에 대해 토큰 수 계산\n",
    "\n",
    "print(\"Token Count (for each chunk):\")\n",
    "print(token_list[:20])  # 처음 20개 토큰 수 출력\n",
    "print(\"=\"*100)\n",
    "\n",
    "# 상위 50개 청크 선택\n",
    "top_50_splits_RCTS_tiktoken = splits_RCTS_tiktoken[:5]  # 순차적으로 상위 50개 청크 선택(제출용 5개)\n",
    "\n",
    "# 상위 50개 청크 출력\n",
    "for i, chunk in enumerate(top_50_splits_RCTS_tiktoken):\n",
    "    print(f\"Chunk {i + 1}: {chunk}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73555c26-3137-48cf-af33-989aafe41e1a",
   "metadata": {},
   "source": [
    "#### RecursiveCharacterTextSplitter + 'cl100k_base' tokenizer (OpenAI 최적화)\n",
    "\n",
    "length_function는 텍스트의 길이를 계산하는 함수입니다. tiktoken_len 함수는 tiktoken을 사용하여 토큰의 개수를 기반으로 길이를 계산합니다.\n",
    "\n",
    "이 방식의 중요한 특징은, 텍스트의 의미를 유지하면서도 효율적으로 토큰 수를 조정할 수 있다는 점입니다. 예를 들어, 각 문서나 텍스트가 일정한 크기(토큰 수)에 맞게 나눠져야 하는데, 이때 RecursiveCharacterTextSplitter는 의미 단위에서 자연스럽게 분할점을 찾을 수 있게 도와줍니다. 대규모 데이터나 문서에서 토큰 길이를 최적화하며 분할할 때 유용합니다. 예를 들어, GPT 모델의 입력 제한을 고려하면서 텍스트를 의미 있게 분할할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608f9da7-04fb-44cc-8a64-1c61701bd106",
   "metadata": {},
   "source": [
    "### 04_OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4d61e150-d161-41f3-9d54-27852bb2c5e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "AuthenticationError",
     "evalue": "Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************fcIA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m-----------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAuthenticationError\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# 각 분할된 텍스트에 대해 임베딩 생성\u001b[39;00m\n\u001b[0;32m     14\u001b[0m doc_list \u001b[38;5;241m=\u001b[39m [split\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m splits_RCTS_tiktoken]  \u001b[38;5;66;03m# 텍스트만 추출\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m embeddings_list \u001b[38;5;241m=\u001b[39m \u001b[43membeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# 임베딩 결과 출력 (예시)\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, embedding \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(embeddings_list[:\u001b[38;5;241m5\u001b[39m]):  \u001b[38;5;66;03m# 첫 5개만 출력 예시\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\p310\\lib\\site-packages\\langchain_openai\\embeddings\\base.py:588\u001b[0m, in \u001b[0;36mOpenAIEmbeddings.embed_documents\u001b[1;34m(self, texts, chunk_size)\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[38;5;66;03m# NOTE: to keep things simple, we assume the list may contain texts longer\u001b[39;00m\n\u001b[0;32m    586\u001b[0m \u001b[38;5;66;03m#       than the maximum context and use length-safe embedding function.\u001b[39;00m\n\u001b[0;32m    587\u001b[0m engine \u001b[38;5;241m=\u001b[39m cast(\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeployment)\n\u001b[1;32m--> 588\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_len_safe_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\p310\\lib\\site-packages\\langchain_openai\\embeddings\\base.py:483\u001b[0m, in \u001b[0;36mOpenAIEmbeddings._get_len_safe_embeddings\u001b[1;34m(self, texts, engine, chunk_size)\u001b[0m\n\u001b[0;32m    481\u001b[0m batched_embeddings: List[List[\u001b[38;5;28mfloat\u001b[39m]] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m _iter:\n\u001b[1;32m--> 483\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[0;32m    484\u001b[0m         \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mtokens[i : i \u001b[38;5;241m+\u001b[39m _chunk_size], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_invocation_params\n\u001b[0;32m    485\u001b[0m     )\n\u001b[0;32m    486\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    487\u001b[0m         response \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mmodel_dump()\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\p310\\lib\\site-packages\\openai\\resources\\embeddings.py:124\u001b[0m, in \u001b[0;36mEmbeddings.create\u001b[1;34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    118\u001b[0m         embedding\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfrombuffer(  \u001b[38;5;66;03m# type: ignore[no-untyped-call]\u001b[39;00m\n\u001b[0;32m    119\u001b[0m             base64\u001b[38;5;241m.\u001b[39mb64decode(data), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    120\u001b[0m         )\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[1;32m--> 124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/embeddings\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEmbeddingCreateParams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    131\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpost_parser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    133\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCreateEmbeddingResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\p310\\lib\\site-packages\\openai\\_base_client.py:1278\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1264\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1265\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1266\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1273\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1274\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1275\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1276\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1277\u001b[0m     )\n\u001b[1;32m-> 1278\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\p310\\lib\\site-packages\\openai\\_base_client.py:955\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    952\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    953\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 955\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    956\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    957\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    961\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\p310\\lib\\site-packages\\openai\\_base_client.py:1059\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1056\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m   1058\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1059\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1061\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[0;32m   1062\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1063\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1067\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[0;32m   1068\u001b[0m )\n",
      "\u001b[1;31mAuthenticationError\u001b[0m: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************fcIA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}"
     ]
    }
   ],
   "source": [
    "from langchain.storage import LocalFileStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "\n",
    "# OpenAI 임베딩 모델 초기화\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "\n",
    "# 임베딩 결과를 캐시해서 저장, 과금 절약 CacheBackedEmbeddings\n",
    "store = LocalFileStore(\"./cache/\")\n",
    "cached_embedder = CacheBackedEmbeddings.from_bytes_store(embeddings, store, namespace=embeddings.model)\n",
    "\n",
    "# 각 분할된 텍스트에 대해 임베딩 생성\n",
    "doc_list = [split.page_content for split in splits_RCTS_tiktoken]  # 텍스트만 추출\n",
    "embeddings_list = embeddings.embed_documents(doc_list)\n",
    "\n",
    "\n",
    "# 임베딩 결과 출력 (예시)\n",
    "for idx, embedding in enumerate(embeddings_list[:5]):  # 첫 5개만 출력 예시\n",
    "    print(f\"Embedding {idx + 1}: {embedding[:10]}...\")  # 임베딩의 앞부분만 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbac1a7-187b-4c88-82e0-4a4bbd35b6e2",
   "metadata": {},
   "source": [
    "#### CacheBackedEmbeddings\n",
    "**과금 절약**: 이 방식은 주로 클라우드에서 API 호출 시 비용이 발생하는 경우, 이미 계산된 임베딩을 로컬 캐시에 저장하여, 반복된 계산을 피하고 네트워크 요청을 줄여 과금을 절약하는 데 유용합니다.\n",
    "\n",
    "**로컬 캐시 활용**: LocalFileStore를 사용해 캐시를 로컬에 저장하므로, 같은 데이터에 대해 다시 계산할 필요가 없어 성능을 향상시키고 리소스를 절약할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31800f0d-d6ea-4270-a4a7-0416ca4d5a08",
   "metadata": {},
   "source": [
    "### 05_Vectorstores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "54195391-4ffc-46ae-a557-2a75a8b3a955",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from langchain_community.vectorstores import Chroma, FAISS\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "\n",
    "# Chroma 클라이언트 설정\n",
    "client = chromadb.Client()\n",
    "\n",
    "# FAISS VectorStore 생성\n",
    "faiss_vectorstore = FAISS.from_documents(documents=splits_RCTS_tiktoken, embedding=embeddings)\n",
    "\n",
    "# Chroma 클라이언트 설정\n",
    "client = chromadb.Client()\n",
    "chroma_vectorstore = Chroma.from_documents(documents=splits_RCTS_tiktoken, embedding=embeddings, client=client)\n",
    "\n",
    "# BM25 retriever 생성\n",
    "bm25_retriever = BM25Retriever.from_texts(doc_list)\n",
    "bm25_retriever.k = 3\n",
    "\n",
    "# Chroma retriever 생성\n",
    "chroma_retriever = chroma_vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "# FAISS retriever 생성\n",
    "faiss_retriever = faiss_vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "# EnsembleRetriever 생성 (FAISS, Chroma, 결과 결합)\n",
    "ensemble_01_retriever = EnsembleRetriever(\n",
    "    retrievers=[faiss_retriever, chroma_retriever],\n",
    "    weight=[0.5, 0.5]\n",
    ")\n",
    "\n",
    "# EnsembleRetriever 생성 (FAISS, BM25 결과 결합)\n",
    "ensemble_02_retriever = EnsembleRetriever(\n",
    "    retrievers=[faiss_retriever, bm25_retriever],\n",
    "    weight=[0.5, 0.5]\n",
    ")\n",
    "\n",
    "# EnsembleRetriever 생성 (Chroma, BM25 결과 결합)\n",
    "ensemble_03_retriever = EnsembleRetriever(\n",
    "    retrievers=[chroma_retriever, bm25_retriever],\n",
    "    weight=[0.5, 0.5]\n",
    ")\n",
    "\n",
    "# EnsembleRetriever 생성 (Chroma, FAISS, BM25 결과 결합)\n",
    "ensemble_04_retriever = EnsembleRetriever(\n",
    "    retrievers=[bm25_retriever, chroma_retriever, faiss_retriever],\n",
    "    weight=[0.33, 0.33, 0.33]  # 세 가지 retriever의 결과를 동일한 비율로 결합\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6517abf-5fb7-46d3-9b74-3f193d50c9d4",
   "metadata": {},
   "source": [
    "### 05_01_Vectorstores_Output(생략)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d0a728fe-0433-4b09-b548-a52b4fdd36d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FAISS Retriever]\n",
      "Content: 아키텍처이지만, 다중 에이전트 생태계 조성  측면에서는 한계를 내포 ∙트랜스포머 아키텍처는 처리하는 컨텍스트가 길수록 속도가 느리고 연산 비용이 많이 드는데, AI  에이전트는 LLM을 여러 차례 호출해야 하고 각 단계에서 광범위한 컨텍스트를 사용하는 경우가 많아  처리 과정에서 지연이 발생 n 고센 CEO는 ‘맘바(Mamba)’와 ‘잠바(Jamba)’와 같은 대체 아키텍처를 활용하면 AI 에이전트를 더  효율적이고 저렴하게 만들 수 있다고 강조 ∙카네기멜론⼤와 프린스턴⼤ 연구진이 개발한 맘바는 트랜스포머 모델의 핵심인 어텐션(Attention)*  메커니즘 대신 데이터를 우선순위에 따라 정리하고 입력에 가중치를 부여해 메모리 사용을 최적화 * 입력된 데이터 간 연관성을 파악해 상호작용을 계산하는 메커니즘  ∙미스트랄이 2024년 7월 ‘코드스트랄(Codestral) 맘바 7B’를, UAE의 AI 기업 팔콘(Falcon)이 8월  ‘팔콘 맘바 7B’를 출시하는 등, 최근 오픈소스 AI 개발자 사이에서 맘바의 인기가 높아지는 추세 ∙AI21 역시 맘바\n",
      "\n",
      "Content: 1. 정책/법제   2. 기업/산업  3. 기술/연구   4. 인력/교육 7 메타, 동영상 생성AI 도구 ‘메타 무비 젠’ 공개 n 메타가 동영상 생성, 개인화 동영상 제작, 동영상 편집, 오디오 생성과 같은 기능을 지원하는  ‘메타 무비 젠’을 공개하고 2025년 중 인스타그램 등 자사 플랫폼에 통합할 계획 n 메타 무비 젠은 인간 선호도 평가에서 런웨이의 젠 3, 오픈AI의 소라, 클링 1.5와 같은 경쟁  동영상 AI 모델보다 더 높은 점수를 기록 KEY Contents £ 메타, 동영상 제작과 편집, 오디오 생성을 지원하는 메타 무비 젠을 공개  n 메타(Meta)가 2024년 10월 4일 텍스트 입력을 통해 고해상도 동영상을 생성하는 AI 도구 ‘메타  무비 젠(Meta Movie Gen)’을 공개 ∙메타는 크리에이터와 영화 제작자 등 소수의 외부 파트너에게 메타 무비 젠을 우선 제공 후 피드백을  반영해 기능을 개선할 계획으로, 단독 서비스로 출시하는 대신 2025년 중 인스타그램(Instagram)과  같은 자사 소셜미디어 플랫폼에 통합하여 제공할 방침 n\n",
      "\n",
      "Content: 없이 개인정보보호가 가능한 온디바이스 추론을 원하는 고객 수요에 맞게  지연시간이 짧고 효율적인 솔루션을 제공한다고 강조 ∙미스트랄AI는 8B 버전만 연구용으로 다운로드를 허용했으며 향후 두 모델을 클라우드 플랫폼을 통해  제공할 계획으로, 사용 비용은 100만 입출력 토큰 당 8B 버전은 10센트, 3B 버전은 4센트로 책정 £ 레 미니스트로, 오픈소스 모델 ‘젬마’ 및 ‘라마’ 대비 대부분 벤치마크에서 우수한 평가 n 벤치마크 평가 결과, 레 미니스트로는 비슷한 매개변수를 가진 오픈소스 모델 젬마(Gemma)와 라마 (Llama)보다 대부분 벤치마크에서 더 높은 평가를 획득 ∙MMLU* 평가에서 미스트랄 3B는 60.9점을 얻어 구글의 ‘젬마 2 2B’(52.4점)와 메타의 ‘라마 3.2  3B’(56.2점)를 앞섰고, 미스트랄 8B는 65.0점으로 ‘라마 3.1 8B’(64.7점)와 1년 전 출시된 자체  모델 ‘미스트랄 7B’(62.5점)를 능가  * 다양한 주제에 대한 모델의 광범위한\n",
      "\n",
      "[chroma_retriever]\n",
      "Content: 아키텍처이지만, 다중 에이전트 생태계 조성  측면에서는 한계를 내포 ∙트랜스포머 아키텍처는 처리하는 컨텍스트가 길수록 속도가 느리고 연산 비용이 많이 드는데, AI  에이전트는 LLM을 여러 차례 호출해야 하고 각 단계에서 광범위한 컨텍스트를 사용하는 경우가 많아  처리 과정에서 지연이 발생 n 고센 CEO는 ‘맘바(Mamba)’와 ‘잠바(Jamba)’와 같은 대체 아키텍처를 활용하면 AI 에이전트를 더  효율적이고 저렴하게 만들 수 있다고 강조 ∙카네기멜론⼤와 프린스턴⼤ 연구진이 개발한 맘바는 트랜스포머 모델의 핵심인 어텐션(Attention)*  메커니즘 대신 데이터를 우선순위에 따라 정리하고 입력에 가중치를 부여해 메모리 사용을 최적화 * 입력된 데이터 간 연관성을 파악해 상호작용을 계산하는 메커니즘  ∙미스트랄이 2024년 7월 ‘코드스트랄(Codestral) 맘바 7B’를, UAE의 AI 기업 팔콘(Falcon)이 8월  ‘팔콘 맘바 7B’를 출시하는 등, 최근 오픈소스 AI 개발자 사이에서 맘바의 인기가 높아지는 추세 ∙AI21 역시 맘바\n",
      "\n",
      "Content: 1. 정책/법제   2. 기업/산업  3. 기술/연구   4. 인력/교육 7 메타, 동영상 생성AI 도구 ‘메타 무비 젠’ 공개 n 메타가 동영상 생성, 개인화 동영상 제작, 동영상 편집, 오디오 생성과 같은 기능을 지원하는  ‘메타 무비 젠’을 공개하고 2025년 중 인스타그램 등 자사 플랫폼에 통합할 계획 n 메타 무비 젠은 인간 선호도 평가에서 런웨이의 젠 3, 오픈AI의 소라, 클링 1.5와 같은 경쟁  동영상 AI 모델보다 더 높은 점수를 기록 KEY Contents £ 메타, 동영상 제작과 편집, 오디오 생성을 지원하는 메타 무비 젠을 공개  n 메타(Meta)가 2024년 10월 4일 텍스트 입력을 통해 고해상도 동영상을 생성하는 AI 도구 ‘메타  무비 젠(Meta Movie Gen)’을 공개 ∙메타는 크리에이터와 영화 제작자 등 소수의 외부 파트너에게 메타 무비 젠을 우선 제공 후 피드백을  반영해 기능을 개선할 계획으로, 단독 서비스로 출시하는 대신 2025년 중 인스타그램(Instagram)과  같은 자사 소셜미디어 플랫폼에 통합하여 제공할 방침 n\n",
      "\n",
      "Content: 없이 개인정보보호가 가능한 온디바이스 추론을 원하는 고객 수요에 맞게  지연시간이 짧고 효율적인 솔루션을 제공한다고 강조 ∙미스트랄AI는 8B 버전만 연구용으로 다운로드를 허용했으며 향후 두 모델을 클라우드 플랫폼을 통해  제공할 계획으로, 사용 비용은 100만 입출력 토큰 당 8B 버전은 10센트, 3B 버전은 4센트로 책정 £ 레 미니스트로, 오픈소스 모델 ‘젬마’ 및 ‘라마’ 대비 대부분 벤치마크에서 우수한 평가 n 벤치마크 평가 결과, 레 미니스트로는 비슷한 매개변수를 가진 오픈소스 모델 젬마(Gemma)와 라마 (Llama)보다 대부분 벤치마크에서 더 높은 평가를 획득 ∙MMLU* 평가에서 미스트랄 3B는 60.9점을 얻어 구글의 ‘젬마 2 2B’(52.4점)와 메타의 ‘라마 3.2  3B’(56.2점)를 앞섰고, 미스트랄 8B는 65.0점으로 ‘라마 3.1 8B’(64.7점)와 1년 전 출시된 자체  모델 ‘미스트랄 7B’(62.5점)를 능가  * 다양한 주제에 대한 모델의 광범위한\n",
      "\n",
      "[BM25 Retriever]\n",
      "Content: Ⅰ. 인공지능 산업 동향 브리프\n",
      "\n",
      "Content: 1. 정책/법제   2. 기업/산업  3. 기술/연구   4. 인력/교육 7 메타, 동영상 생성AI 도구 ‘메타 무비 젠’ 공개 n 메타가 동영상 생성, 개인화 동영상 제작, 동영상 편집, 오디오 생성과 같은 기능을 지원하는  ‘메타 무비 젠’을 공개하고 2025년 중 인스타그램 등 자사 플랫폼에 통합할 계획 n 메타 무비 젠은 인간 선호도 평가에서 런웨이의 젠 3, 오픈AI의 소라, 클링 1.5와 같은 경쟁  동영상 AI 모델보다 더 높은 점수를 기록 KEY Contents £ 메타, 동영상 제작과 편집, 오디오 생성을 지원하는 메타 무비 젠을 공개  n 메타(Meta)가 2024년 10월 4일 텍스트 입력을 통해 고해상도 동영상을 생성하는 AI 도구 ‘메타  무비 젠(Meta Movie Gen)’을 공개 ∙메타는 크리에이터와 영화 제작자 등 소수의 외부 파트너에게 메타 무비 젠을 우선 제공 후 피드백을  반영해 기능을 개선할 계획으로, 단독 서비스로 출시하는 대신 2025년 중 인스타그램(Instagram)과  같은 자사 소셜미디어 플랫폼에 통합하여 제공할 방침 n\n",
      "\n",
      "Content: 메타 무비 젠은 △동영상 생성 △개인화 동영상 생성 △동영상 편집 △오디오 생성의 4가지 기능을 지원 ∙(동영상 생성) 300억 개 매개변수의 AI 모델을 통해 초당 16프레임의 속도로 1,080p 해상도의 최대  16초 길이 동영상 생성을 지원 ∙(개인화 동영상 생성) 사용자가 자신이나 타인의 이미지와 텍스트를 입력해 원래 인물의 고유한 특징을  반영한 개인화 동영상을 제작 가능 ∙(동영상 편집) 특정 요소의 추가나 제거, 변경과 같은 부분적 수정 및 동영상 배경 또는 스타일 변경과  같은 광범위한 수정도 지원 ∙(오디오 생성) 130억 개 매개변수의 오디오 생성 모델을 통합해 동영상과 텍스트 프롬프트 기반으로  최대 45초 길이의 배경음, 음향 효과 등 고품질 오디오를 생성 £ 메타 무비 젠, 인간 선호도 평가에서 오픈AI의 소라 능가 n 메타 무비 젠은 인간 선호도 평가에서 런웨이(Runway)의 젠(Gen) 3, 오픈AI의 소라(Sora)를  비롯한 경쟁 동영상 생성AI 모델보다 더 높은 점수를 기록 ∙메타 무비 젠과 경쟁 모델에 대하여 세 명의 인간\n",
      "\n",
      "[FAISS, Chroma Ensemble Retriever]\n",
      "Content: 아키텍처이지만, 다중 에이전트 생태계 조성  측면에서는 한계를 내포 ∙트랜스포머 아키텍처는 처리하는 컨텍스트가 길수록 속도가 느리고 연산 비용이 많이 드는데, AI  에이전트는 LLM을 여러 차례 호출해야 하고 각 단계에서 광범위한 컨텍스트를 사용하는 경우가 많아  처리 과정에서 지연이 발생 n 고센 CEO는 ‘맘바(Mamba)’와 ‘잠바(Jamba)’와 같은 대체 아키텍처를 활용하면 AI 에이전트를 더  효율적이고 저렴하게 만들 수 있다고 강조 ∙카네기멜론⼤와 프린스턴⼤ 연구진이 개발한 맘바는 트랜스포머 모델의 핵심인 어텐션(Attention)*  메커니즘 대신 데이터를 우선순위에 따라 정리하고 입력에 가중치를 부여해 메모리 사용을 최적화 * 입력된 데이터 간 연관성을 파악해 상호작용을 계산하는 메커니즘  ∙미스트랄이 2024년 7월 ‘코드스트랄(Codestral) 맘바 7B’를, UAE의 AI 기업 팔콘(Falcon)이 8월  ‘팔콘 맘바 7B’를 출시하는 등, 최근 오픈소스 AI 개발자 사이에서 맘바의 인기가 높아지는 추세 ∙AI21 역시 맘바\n",
      "\n",
      "Content: 1. 정책/법제   2. 기업/산업  3. 기술/연구   4. 인력/교육 7 메타, 동영상 생성AI 도구 ‘메타 무비 젠’ 공개 n 메타가 동영상 생성, 개인화 동영상 제작, 동영상 편집, 오디오 생성과 같은 기능을 지원하는  ‘메타 무비 젠’을 공개하고 2025년 중 인스타그램 등 자사 플랫폼에 통합할 계획 n 메타 무비 젠은 인간 선호도 평가에서 런웨이의 젠 3, 오픈AI의 소라, 클링 1.5와 같은 경쟁  동영상 AI 모델보다 더 높은 점수를 기록 KEY Contents £ 메타, 동영상 제작과 편집, 오디오 생성을 지원하는 메타 무비 젠을 공개  n 메타(Meta)가 2024년 10월 4일 텍스트 입력을 통해 고해상도 동영상을 생성하는 AI 도구 ‘메타  무비 젠(Meta Movie Gen)’을 공개 ∙메타는 크리에이터와 영화 제작자 등 소수의 외부 파트너에게 메타 무비 젠을 우선 제공 후 피드백을  반영해 기능을 개선할 계획으로, 단독 서비스로 출시하는 대신 2025년 중 인스타그램(Instagram)과  같은 자사 소셜미디어 플랫폼에 통합하여 제공할 방침 n\n",
      "\n",
      "Content: 없이 개인정보보호가 가능한 온디바이스 추론을 원하는 고객 수요에 맞게  지연시간이 짧고 효율적인 솔루션을 제공한다고 강조 ∙미스트랄AI는 8B 버전만 연구용으로 다운로드를 허용했으며 향후 두 모델을 클라우드 플랫폼을 통해  제공할 계획으로, 사용 비용은 100만 입출력 토큰 당 8B 버전은 10센트, 3B 버전은 4센트로 책정 £ 레 미니스트로, 오픈소스 모델 ‘젬마’ 및 ‘라마’ 대비 대부분 벤치마크에서 우수한 평가 n 벤치마크 평가 결과, 레 미니스트로는 비슷한 매개변수를 가진 오픈소스 모델 젬마(Gemma)와 라마 (Llama)보다 대부분 벤치마크에서 더 높은 평가를 획득 ∙MMLU* 평가에서 미스트랄 3B는 60.9점을 얻어 구글의 ‘젬마 2 2B’(52.4점)와 메타의 ‘라마 3.2  3B’(56.2점)를 앞섰고, 미스트랄 8B는 65.0점으로 ‘라마 3.1 8B’(64.7점)와 1년 전 출시된 자체  모델 ‘미스트랄 7B’(62.5점)를 능가  * 다양한 주제에 대한 모델의 광범위한\n",
      "\n",
      "[FAISS, BM25 Ensemble Retriever]\n",
      "Content: 1. 정책/법제   2. 기업/산업  3. 기술/연구   4. 인력/교육 7 메타, 동영상 생성AI 도구 ‘메타 무비 젠’ 공개 n 메타가 동영상 생성, 개인화 동영상 제작, 동영상 편집, 오디오 생성과 같은 기능을 지원하는  ‘메타 무비 젠’을 공개하고 2025년 중 인스타그램 등 자사 플랫폼에 통합할 계획 n 메타 무비 젠은 인간 선호도 평가에서 런웨이의 젠 3, 오픈AI의 소라, 클링 1.5와 같은 경쟁  동영상 AI 모델보다 더 높은 점수를 기록 KEY Contents £ 메타, 동영상 제작과 편집, 오디오 생성을 지원하는 메타 무비 젠을 공개  n 메타(Meta)가 2024년 10월 4일 텍스트 입력을 통해 고해상도 동영상을 생성하는 AI 도구 ‘메타  무비 젠(Meta Movie Gen)’을 공개 ∙메타는 크리에이터와 영화 제작자 등 소수의 외부 파트너에게 메타 무비 젠을 우선 제공 후 피드백을  반영해 기능을 개선할 계획으로, 단독 서비스로 출시하는 대신 2025년 중 인스타그램(Instagram)과  같은 자사 소셜미디어 플랫폼에 통합하여 제공할 방침 n\n",
      "\n",
      "Content: 아키텍처이지만, 다중 에이전트 생태계 조성  측면에서는 한계를 내포 ∙트랜스포머 아키텍처는 처리하는 컨텍스트가 길수록 속도가 느리고 연산 비용이 많이 드는데, AI  에이전트는 LLM을 여러 차례 호출해야 하고 각 단계에서 광범위한 컨텍스트를 사용하는 경우가 많아  처리 과정에서 지연이 발생 n 고센 CEO는 ‘맘바(Mamba)’와 ‘잠바(Jamba)’와 같은 대체 아키텍처를 활용하면 AI 에이전트를 더  효율적이고 저렴하게 만들 수 있다고 강조 ∙카네기멜론⼤와 프린스턴⼤ 연구진이 개발한 맘바는 트랜스포머 모델의 핵심인 어텐션(Attention)*  메커니즘 대신 데이터를 우선순위에 따라 정리하고 입력에 가중치를 부여해 메모리 사용을 최적화 * 입력된 데이터 간 연관성을 파악해 상호작용을 계산하는 메커니즘  ∙미스트랄이 2024년 7월 ‘코드스트랄(Codestral) 맘바 7B’를, UAE의 AI 기업 팔콘(Falcon)이 8월  ‘팔콘 맘바 7B’를 출시하는 등, 최근 오픈소스 AI 개발자 사이에서 맘바의 인기가 높아지는 추세 ∙AI21 역시 맘바\n",
      "\n",
      "Content: Ⅰ. 인공지능 산업 동향 브리프\n",
      "\n",
      "Content: 없이 개인정보보호가 가능한 온디바이스 추론을 원하는 고객 수요에 맞게  지연시간이 짧고 효율적인 솔루션을 제공한다고 강조 ∙미스트랄AI는 8B 버전만 연구용으로 다운로드를 허용했으며 향후 두 모델을 클라우드 플랫폼을 통해  제공할 계획으로, 사용 비용은 100만 입출력 토큰 당 8B 버전은 10센트, 3B 버전은 4센트로 책정 £ 레 미니스트로, 오픈소스 모델 ‘젬마’ 및 ‘라마’ 대비 대부분 벤치마크에서 우수한 평가 n 벤치마크 평가 결과, 레 미니스트로는 비슷한 매개변수를 가진 오픈소스 모델 젬마(Gemma)와 라마 (Llama)보다 대부분 벤치마크에서 더 높은 평가를 획득 ∙MMLU* 평가에서 미스트랄 3B는 60.9점을 얻어 구글의 ‘젬마 2 2B’(52.4점)와 메타의 ‘라마 3.2  3B’(56.2점)를 앞섰고, 미스트랄 8B는 65.0점으로 ‘라마 3.1 8B’(64.7점)와 1년 전 출시된 자체  모델 ‘미스트랄 7B’(62.5점)를 능가  * 다양한 주제에 대한 모델의 광범위한\n",
      "\n",
      "Content: 메타 무비 젠은 △동영상 생성 △개인화 동영상 생성 △동영상 편집 △오디오 생성의 4가지 기능을 지원 ∙(동영상 생성) 300억 개 매개변수의 AI 모델을 통해 초당 16프레임의 속도로 1,080p 해상도의 최대  16초 길이 동영상 생성을 지원 ∙(개인화 동영상 생성) 사용자가 자신이나 타인의 이미지와 텍스트를 입력해 원래 인물의 고유한 특징을  반영한 개인화 동영상을 제작 가능 ∙(동영상 편집) 특정 요소의 추가나 제거, 변경과 같은 부분적 수정 및 동영상 배경 또는 스타일 변경과  같은 광범위한 수정도 지원 ∙(오디오 생성) 130억 개 매개변수의 오디오 생성 모델을 통합해 동영상과 텍스트 프롬프트 기반으로  최대 45초 길이의 배경음, 음향 효과 등 고품질 오디오를 생성 £ 메타 무비 젠, 인간 선호도 평가에서 오픈AI의 소라 능가 n 메타 무비 젠은 인간 선호도 평가에서 런웨이(Runway)의 젠(Gen) 3, 오픈AI의 소라(Sora)를  비롯한 경쟁 동영상 생성AI 모델보다 더 높은 점수를 기록 ∙메타 무비 젠과 경쟁 모델에 대하여 세 명의 인간\n",
      "\n",
      "[Chroma, BM25 Ensemble Retriever]\n",
      "Content: 1. 정책/법제   2. 기업/산업  3. 기술/연구   4. 인력/교육 7 메타, 동영상 생성AI 도구 ‘메타 무비 젠’ 공개 n 메타가 동영상 생성, 개인화 동영상 제작, 동영상 편집, 오디오 생성과 같은 기능을 지원하는  ‘메타 무비 젠’을 공개하고 2025년 중 인스타그램 등 자사 플랫폼에 통합할 계획 n 메타 무비 젠은 인간 선호도 평가에서 런웨이의 젠 3, 오픈AI의 소라, 클링 1.5와 같은 경쟁  동영상 AI 모델보다 더 높은 점수를 기록 KEY Contents £ 메타, 동영상 제작과 편집, 오디오 생성을 지원하는 메타 무비 젠을 공개  n 메타(Meta)가 2024년 10월 4일 텍스트 입력을 통해 고해상도 동영상을 생성하는 AI 도구 ‘메타  무비 젠(Meta Movie Gen)’을 공개 ∙메타는 크리에이터와 영화 제작자 등 소수의 외부 파트너에게 메타 무비 젠을 우선 제공 후 피드백을  반영해 기능을 개선할 계획으로, 단독 서비스로 출시하는 대신 2025년 중 인스타그램(Instagram)과  같은 자사 소셜미디어 플랫폼에 통합하여 제공할 방침 n\n",
      "\n",
      "Content: 아키텍처이지만, 다중 에이전트 생태계 조성  측면에서는 한계를 내포 ∙트랜스포머 아키텍처는 처리하는 컨텍스트가 길수록 속도가 느리고 연산 비용이 많이 드는데, AI  에이전트는 LLM을 여러 차례 호출해야 하고 각 단계에서 광범위한 컨텍스트를 사용하는 경우가 많아  처리 과정에서 지연이 발생 n 고센 CEO는 ‘맘바(Mamba)’와 ‘잠바(Jamba)’와 같은 대체 아키텍처를 활용하면 AI 에이전트를 더  효율적이고 저렴하게 만들 수 있다고 강조 ∙카네기멜론⼤와 프린스턴⼤ 연구진이 개발한 맘바는 트랜스포머 모델의 핵심인 어텐션(Attention)*  메커니즘 대신 데이터를 우선순위에 따라 정리하고 입력에 가중치를 부여해 메모리 사용을 최적화 * 입력된 데이터 간 연관성을 파악해 상호작용을 계산하는 메커니즘  ∙미스트랄이 2024년 7월 ‘코드스트랄(Codestral) 맘바 7B’를, UAE의 AI 기업 팔콘(Falcon)이 8월  ‘팔콘 맘바 7B’를 출시하는 등, 최근 오픈소스 AI 개발자 사이에서 맘바의 인기가 높아지는 추세 ∙AI21 역시 맘바\n",
      "\n",
      "Content: Ⅰ. 인공지능 산업 동향 브리프\n",
      "\n",
      "Content: 없이 개인정보보호가 가능한 온디바이스 추론을 원하는 고객 수요에 맞게  지연시간이 짧고 효율적인 솔루션을 제공한다고 강조 ∙미스트랄AI는 8B 버전만 연구용으로 다운로드를 허용했으며 향후 두 모델을 클라우드 플랫폼을 통해  제공할 계획으로, 사용 비용은 100만 입출력 토큰 당 8B 버전은 10센트, 3B 버전은 4센트로 책정 £ 레 미니스트로, 오픈소스 모델 ‘젬마’ 및 ‘라마’ 대비 대부분 벤치마크에서 우수한 평가 n 벤치마크 평가 결과, 레 미니스트로는 비슷한 매개변수를 가진 오픈소스 모델 젬마(Gemma)와 라마 (Llama)보다 대부분 벤치마크에서 더 높은 평가를 획득 ∙MMLU* 평가에서 미스트랄 3B는 60.9점을 얻어 구글의 ‘젬마 2 2B’(52.4점)와 메타의 ‘라마 3.2  3B’(56.2점)를 앞섰고, 미스트랄 8B는 65.0점으로 ‘라마 3.1 8B’(64.7점)와 1년 전 출시된 자체  모델 ‘미스트랄 7B’(62.5점)를 능가  * 다양한 주제에 대한 모델의 광범위한\n",
      "\n",
      "Content: 메타 무비 젠은 △동영상 생성 △개인화 동영상 생성 △동영상 편집 △오디오 생성의 4가지 기능을 지원 ∙(동영상 생성) 300억 개 매개변수의 AI 모델을 통해 초당 16프레임의 속도로 1,080p 해상도의 최대  16초 길이 동영상 생성을 지원 ∙(개인화 동영상 생성) 사용자가 자신이나 타인의 이미지와 텍스트를 입력해 원래 인물의 고유한 특징을  반영한 개인화 동영상을 제작 가능 ∙(동영상 편집) 특정 요소의 추가나 제거, 변경과 같은 부분적 수정 및 동영상 배경 또는 스타일 변경과  같은 광범위한 수정도 지원 ∙(오디오 생성) 130억 개 매개변수의 오디오 생성 모델을 통합해 동영상과 텍스트 프롬프트 기반으로  최대 45초 길이의 배경음, 음향 효과 등 고품질 오디오를 생성 £ 메타 무비 젠, 인간 선호도 평가에서 오픈AI의 소라 능가 n 메타 무비 젠은 인간 선호도 평가에서 런웨이(Runway)의 젠(Gen) 3, 오픈AI의 소라(Sora)를  비롯한 경쟁 동영상 생성AI 모델보다 더 높은 점수를 기록 ∙메타 무비 젠과 경쟁 모델에 대하여 세 명의 인간\n",
      "\n",
      "[Chroma, FAISS, BM25 Ensemble Retriever]\n",
      "Content: 1. 정책/법제   2. 기업/산업  3. 기술/연구   4. 인력/교육 7 메타, 동영상 생성AI 도구 ‘메타 무비 젠’ 공개 n 메타가 동영상 생성, 개인화 동영상 제작, 동영상 편집, 오디오 생성과 같은 기능을 지원하는  ‘메타 무비 젠’을 공개하고 2025년 중 인스타그램 등 자사 플랫폼에 통합할 계획 n 메타 무비 젠은 인간 선호도 평가에서 런웨이의 젠 3, 오픈AI의 소라, 클링 1.5와 같은 경쟁  동영상 AI 모델보다 더 높은 점수를 기록 KEY Contents £ 메타, 동영상 제작과 편집, 오디오 생성을 지원하는 메타 무비 젠을 공개  n 메타(Meta)가 2024년 10월 4일 텍스트 입력을 통해 고해상도 동영상을 생성하는 AI 도구 ‘메타  무비 젠(Meta Movie Gen)’을 공개 ∙메타는 크리에이터와 영화 제작자 등 소수의 외부 파트너에게 메타 무비 젠을 우선 제공 후 피드백을  반영해 기능을 개선할 계획으로, 단독 서비스로 출시하는 대신 2025년 중 인스타그램(Instagram)과  같은 자사 소셜미디어 플랫폼에 통합하여 제공할 방침 n\n",
      "\n",
      "Content: 아키텍처이지만, 다중 에이전트 생태계 조성  측면에서는 한계를 내포 ∙트랜스포머 아키텍처는 처리하는 컨텍스트가 길수록 속도가 느리고 연산 비용이 많이 드는데, AI  에이전트는 LLM을 여러 차례 호출해야 하고 각 단계에서 광범위한 컨텍스트를 사용하는 경우가 많아  처리 과정에서 지연이 발생 n 고센 CEO는 ‘맘바(Mamba)’와 ‘잠바(Jamba)’와 같은 대체 아키텍처를 활용하면 AI 에이전트를 더  효율적이고 저렴하게 만들 수 있다고 강조 ∙카네기멜론⼤와 프린스턴⼤ 연구진이 개발한 맘바는 트랜스포머 모델의 핵심인 어텐션(Attention)*  메커니즘 대신 데이터를 우선순위에 따라 정리하고 입력에 가중치를 부여해 메모리 사용을 최적화 * 입력된 데이터 간 연관성을 파악해 상호작용을 계산하는 메커니즘  ∙미스트랄이 2024년 7월 ‘코드스트랄(Codestral) 맘바 7B’를, UAE의 AI 기업 팔콘(Falcon)이 8월  ‘팔콘 맘바 7B’를 출시하는 등, 최근 오픈소스 AI 개발자 사이에서 맘바의 인기가 높아지는 추세 ∙AI21 역시 맘바\n",
      "\n",
      "Content: 없이 개인정보보호가 가능한 온디바이스 추론을 원하는 고객 수요에 맞게  지연시간이 짧고 효율적인 솔루션을 제공한다고 강조 ∙미스트랄AI는 8B 버전만 연구용으로 다운로드를 허용했으며 향후 두 모델을 클라우드 플랫폼을 통해  제공할 계획으로, 사용 비용은 100만 입출력 토큰 당 8B 버전은 10센트, 3B 버전은 4센트로 책정 £ 레 미니스트로, 오픈소스 모델 ‘젬마’ 및 ‘라마’ 대비 대부분 벤치마크에서 우수한 평가 n 벤치마크 평가 결과, 레 미니스트로는 비슷한 매개변수를 가진 오픈소스 모델 젬마(Gemma)와 라마 (Llama)보다 대부분 벤치마크에서 더 높은 평가를 획득 ∙MMLU* 평가에서 미스트랄 3B는 60.9점을 얻어 구글의 ‘젬마 2 2B’(52.4점)와 메타의 ‘라마 3.2  3B’(56.2점)를 앞섰고, 미스트랄 8B는 65.0점으로 ‘라마 3.1 8B’(64.7점)와 1년 전 출시된 자체  모델 ‘미스트랄 7B’(62.5점)를 능가  * 다양한 주제에 대한 모델의 광범위한\n",
      "\n",
      "Content: Ⅰ. 인공지능 산업 동향 브리프\n",
      "\n",
      "Content: 메타 무비 젠은 △동영상 생성 △개인화 동영상 생성 △동영상 편집 △오디오 생성의 4가지 기능을 지원 ∙(동영상 생성) 300억 개 매개변수의 AI 모델을 통해 초당 16프레임의 속도로 1,080p 해상도의 최대  16초 길이 동영상 생성을 지원 ∙(개인화 동영상 생성) 사용자가 자신이나 타인의 이미지와 텍스트를 입력해 원래 인물의 고유한 특징을  반영한 개인화 동영상을 제작 가능 ∙(동영상 편집) 특정 요소의 추가나 제거, 변경과 같은 부분적 수정 및 동영상 배경 또는 스타일 변경과  같은 광범위한 수정도 지원 ∙(오디오 생성) 130억 개 매개변수의 오디오 생성 모델을 통합해 동영상과 텍스트 프롬프트 기반으로  최대 45초 길이의 배경음, 음향 효과 등 고품질 오디오를 생성 £ 메타 무비 젠, 인간 선호도 평가에서 오픈AI의 소라 능가 n 메타 무비 젠은 인간 선호도 평가에서 런웨이(Runway)의 젠(Gen) 3, 오픈AI의 소라(Sora)를  비롯한 경쟁 동영상 생성AI 모델보다 더 높은 점수를 기록 ∙메타 무비 젠과 경쟁 모델에 대하여 세 명의 인간\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 검색 결과 문서를 가져옵니다.\n",
    "query = \"메타 인공지능\"\n",
    "\n",
    "faiss_result = faiss_retriever.invoke(query)\n",
    "chroma_result = chroma_retriever.invoke(query)\n",
    "bm25_result = bm25_retriever.invoke(query)\n",
    "ensemble_01_result = ensemble_01_retriever.invoke(query)\n",
    "ensemble_02_result = ensemble_02_retriever.invoke(query)\n",
    "ensemble_03_result = ensemble_03_retriever.invoke(query)\n",
    "ensemble_04_result = ensemble_04_retriever.invoke(query)\n",
    "\n",
    "print(\"[FAISS Retriever]\")\n",
    "for doc in faiss_result:\n",
    "    print(f\"Content: {doc.page_content}\")\n",
    "    print()\n",
    "\n",
    "print(\"[chroma_retriever]\")\n",
    "for doc in chroma_result:\n",
    "    print(f\"Content: {doc.page_content}\")\n",
    "    print()\n",
    "\n",
    "print(\"[BM25 Retriever]\")\n",
    "for doc in bm25_result:\n",
    "    print(f\"Content: {doc.page_content}\")\n",
    "    print()\n",
    "\n",
    "# 가져온 문서를 출력합니다.\n",
    "print(\"[FAISS, Chroma Ensemble Retriever]\")\n",
    "for doc in ensemble_01_result:\n",
    "    print(f\"Content: {doc.page_content}\")\n",
    "    print()\n",
    "\n",
    "print(\"[FAISS, BM25 Ensemble Retriever]\")\n",
    "for doc in ensemble_02_result:\n",
    "    print(f\"Content: {doc.page_content}\")\n",
    "    print()\n",
    "\n",
    "print(\"[Chroma, BM25 Ensemble Retriever]\")\n",
    "for doc in ensemble_03_result:\n",
    "    print(f\"Content: {doc.page_content}\")\n",
    "    print()\n",
    "\n",
    "print(\"[Chroma, FAISS, BM25 Ensemble Retriever]\")\n",
    "for doc in ensemble_04_result:\n",
    "    print(f\"Content: {doc.page_content}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ba2fe4-3e41-49d6-b734-7b4217f8ca2d",
   "metadata": {},
   "source": [
    "#### 단독\n",
    "\n",
    "**FAISS** : 문서의 임베딩 벡터를 사용하여 의미적 유사성을 기반으로 검색합니다. 즉, 문서의 의미와 관련된 유사한 벡터를 찾아냅니다. FAISS는 대규모 데이터셋에서 고속으로 검색할 수 있는 효율적인 방법입니다.\n",
    "\n",
    "**Chroma**: 벡터 기반의 의미적 유사성 검색을 수행하게 됩니다. Chroma는 임베딩 벡터를 사용하여 문서 간의 의미적 유사성을 계산하고, 쿼리와 가장 유사한 문서를 찾아 반환합니다.\n",
    "\n",
    "**BM25 Retriever** : 주로 정확한 단어 일치를 기반으로 하여, 쿼리와 문서의 키워드 일치 정도에 따라 순위를 매깁니다.\n",
    "\n",
    "#### 앙상블\n",
    "\n",
    "**FAISS + Chroma** : FAISS는 벡터 기반의 의미적 유사성, Chroma는 벡터와 데이터베이스를 사용하여 보다 다양한 의미적 연관성을 찾아냅니다.\n",
    "\n",
    "**FAISS + BM25** : FAISS의 벡터 기반 검색과 BM25의 키워드 기반 검색을 결합하여, 두 방식의 장점을 동시에 취합니다.\n",
    "\n",
    "**Chroma + BM25** : 텍스트의 의미와 키워드를 동시에 고려한 결과를 제공합니다.(**사용**)\n",
    "\n",
    "**FAISS + Chroma + BM25** : FAISS는 의미적 유사성, BM25는 키워드 일치, Chroma는 벡터화된 문서 간 유사성에 기반한 검색을 결합하여, 각 검색 방식이 보완적으로 작용하게 됩니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14abe089-13ff-4c04-8796-67dd4a1caea5",
   "metadata": {},
   "source": [
    "### 06_LLM_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6504218e-0ccc-4836-8106-f05f8992cbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler # 스트리밍 방식으로 출력\n",
    "from langchain_core.messages import HumanMessage\n",
    "# 모델 초기화\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\", \n",
    "                   callbacks=[StreamingStdOutCallbackHandler()], \n",
    "                   streaming=True,\n",
    "                   temperature = 0, # temperature = 0 ~ 2 높을 수록 다양한 답변. 버전에 따라 디폴트 값이 변경될 수 있기 때문에 명시필요.\n",
    "                   max_tokens = 1000\n",
    "                  ) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c1ea8b-236e-4556-aba2-f0de173695af",
   "metadata": {},
   "source": [
    "### 07_LongContextReorder(미사용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8899ab5f-a756-46f9-bb42-e87464cefe64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain, StuffDocumentsChain \n",
    "from langchain_community.document_transformers import LongContextReorder \n",
    "\n",
    "reordering = LongContextReorder()\n",
    "reordered_docs = reordering.transform_documents(ensemble_03_retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee29eee0-cfd2-43e9-9876-5cd0914bbbe9",
   "metadata": {},
   "source": [
    "### 08_RAG+prompt_engineering_load_and_save "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5d00fc09-e7b2-435d-b450-03929e93c802",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "prompt_file = \"prompt_002\"\n",
    "\n",
    "# system_prompt를 /01_prompts/prompt_000.txt 파일에서 읽어오기\n",
    "def load_system_prompt():\n",
    "    with open(f\"01_prompts/{prompt_file}.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "        return file.read().replace(\"\\n\", \" \")  # \\n을 공백으로 대체\n",
    "        \n",
    "# 시스템 프롬프트 로드\n",
    "system_prompt = load_system_prompt()\n",
    "\n",
    "# 프롬프트 템플릿 정의\n",
    "contextual_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"user\", \"Context: {context}\\\\n\\\\nQuestion: {question}\")\n",
    "])\n",
    "\n",
    "class DebugPassThrough(RunnablePassthrough):\n",
    "    def invoke(self, *args, **kwargs):\n",
    "        output = super().invoke(*args, **kwargs)\n",
    "        \n",
    "        # output의 실제 타입 확인\n",
    "        #print(\"Type of output:\", type(output))\n",
    "        #print(\"Output content:\", output)  # output 내용을 확인\n",
    "\n",
    "        # 만약 output이 dict 형태라면\n",
    "        if isinstance(output, dict):\n",
    "            # 문서 내용 출력 (각 문서 구분만 하기)\n",
    "            documents = output.get('context', [])\n",
    "            for idx, doc in enumerate(documents):\n",
    "                # 각 문서의 page_content를 출력\n",
    "                print(f\"Document {idx + 1}:\")\n",
    "                print(doc.page_content)  # Document 객체의 page_content 출력\n",
    "                print(\"========================\")  # 각 문서 구분선\n",
    "        else:\n",
    "            # 만약 output이 문자열이라면, 그냥 출력\n",
    "            #print(\"Received output is not a dictionary, but a string:\")\n",
    "            print(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# 문서 리스트를 텍스트로 변환하는 단계 추가\n",
    "class ContextToText(RunnablePassthrough):\n",
    "    def invoke(self, inputs, config=None, **kwargs):  # config 인수 추가\n",
    "        # context의 각 문서를 문자열로 결합\n",
    "        context_text = \"\\n\".join([doc.page_content for doc in inputs[\"context\"]])\n",
    "        return {\"context\": context_text, \"question\": inputs[\"question\"]}\n",
    "\n",
    "# RAG 체인에서 각 단계마다 DebugPassThrough 추가\n",
    "rag_chain_debug = {\n",
    "    \"context\": ensemble_03_retriever,                    # 컨텍스트를 가져오는 retriever\n",
    "    \"question\": DebugPassThrough()        # 사용자 질문이 그대로 전달되는지 확인하는 passthrough\n",
    "}  | DebugPassThrough() | ContextToText()| contextual_prompt | model\n",
    "\n",
    "# 출력 결과와 질문을 /02_results/ 폴더에 저장하는 함수\n",
    "def save_output_to_file(question, response):\n",
    "    # 현재 시간을 timestamp 형식으로 기록\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    # 결과를 저장할 파일 경로\n",
    "    output_filename = f\"02_results/{prompt_file}_{question}.txt\"\n",
    "    \n",
    "    # 파일을 생성하고 저장\n",
    "    with open(output_filename, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(f\"시스템: {system_prompt}\\n\\n\")\n",
    "        file.write(f\"질문: {question}\\n\\n\")\n",
    "        file.write(f\"답변: {response.content}\\n\\n\")\n",
    "        file.write(f\"타임스탬프: {timestamp}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43638ef7-4e12-4603-87fb-e1d3b9e291da",
   "metadata": {},
   "source": [
    "### 09_RAG_답변"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "197b1a9c-d87e-4601-9f37-10ed2e2315ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "메타의 인공지능에 대해 알려줘\n",
      "Document 1:\n",
      "아키텍처이지만, 다중 에이전트 생태계 조성  측면에서는 한계를 내포 ∙트랜스포머 아키텍처는 처리하는 컨텍스트가 길수록 속도가 느리고 연산 비용이 많이 드는데, AI  에이전트는 LLM을 여러 차례 호출해야 하고 각 단계에서 광범위한 컨텍스트를 사용하는 경우가 많아  처리 과정에서 지연이 발생 n 고센 CEO는 ‘맘바(Mamba)’와 ‘잠바(Jamba)’와 같은 대체 아키텍처를 활용하면 AI 에이전트를 더  효율적이고 저렴하게 만들 수 있다고 강조 ∙카네기멜론⼤와 프린스턴⼤ 연구진이 개발한 맘바는 트랜스포머 모델의 핵심인 어텐션(Attention)*  메커니즘 대신 데이터를 우선순위에 따라 정리하고 입력에 가중치를 부여해 메모리 사용을 최적화 * 입력된 데이터 간 연관성을 파악해 상호작용을 계산하는 메커니즘  ∙미스트랄이 2024년 7월 ‘코드스트랄(Codestral) 맘바 7B’를, UAE의 AI 기업 팔콘(Falcon)이 8월  ‘팔콘 맘바 7B’를 출시하는 등, 최근 오픈소스 AI 개발자 사이에서 맘바의 인기가 높아지는 추세 ∙AI21 역시 맘바\n",
      "========================\n",
      "Document 2:\n",
      "없이 개인정보보호가 가능한 온디바이스 추론을 원하는 고객 수요에 맞게  지연시간이 짧고 효율적인 솔루션을 제공한다고 강조 ∙미스트랄AI는 8B 버전만 연구용으로 다운로드를 허용했으며 향후 두 모델을 클라우드 플랫폼을 통해  제공할 계획으로, 사용 비용은 100만 입출력 토큰 당 8B 버전은 10센트, 3B 버전은 4센트로 책정 £ 레 미니스트로, 오픈소스 모델 ‘젬마’ 및 ‘라마’ 대비 대부분 벤치마크에서 우수한 평가 n 벤치마크 평가 결과, 레 미니스트로는 비슷한 매개변수를 가진 오픈소스 모델 젬마(Gemma)와 라마 (Llama)보다 대부분 벤치마크에서 더 높은 평가를 획득 ∙MMLU* 평가에서 미스트랄 3B는 60.9점을 얻어 구글의 ‘젬마 2 2B’(52.4점)와 메타의 ‘라마 3.2  3B’(56.2점)를 앞섰고, 미스트랄 8B는 65.0점으로 ‘라마 3.1 8B’(64.7점)와 1년 전 출시된 자체  모델 ‘미스트랄 7B’(62.5점)를 능가  * 다양한 주제에 대한 모델의 광범위한\n",
      "========================\n",
      "Document 3:\n",
      "1. 정책/법제   2. 기업/산업  3. 기술/연구   4. 인력/교육 7 메타, 동영상 생성AI 도구 ‘메타 무비 젠’ 공개 n 메타가 동영상 생성, 개인화 동영상 제작, 동영상 편집, 오디오 생성과 같은 기능을 지원하는  ‘메타 무비 젠’을 공개하고 2025년 중 인스타그램 등 자사 플랫폼에 통합할 계획 n 메타 무비 젠은 인간 선호도 평가에서 런웨이의 젠 3, 오픈AI의 소라, 클링 1.5와 같은 경쟁  동영상 AI 모델보다 더 높은 점수를 기록 KEY Contents £ 메타, 동영상 제작과 편집, 오디오 생성을 지원하는 메타 무비 젠을 공개  n 메타(Meta)가 2024년 10월 4일 텍스트 입력을 통해 고해상도 동영상을 생성하는 AI 도구 ‘메타  무비 젠(Meta Movie Gen)’을 공개 ∙메타는 크리에이터와 영화 제작자 등 소수의 외부 파트너에게 메타 무비 젠을 우선 제공 후 피드백을  반영해 기능을 개선할 계획으로, 단독 서비스로 출시하는 대신 2025년 중 인스타그램(Instagram)과  같은 자사 소셜미디어 플랫폼에 통합하여 제공할 방침 n\n",
      "========================\n",
      "Document 4:\n",
      "유해 정보의 출력을 통제 가능 허위 정보와  조작 방지 인간중심, 안전성, 투명성 Ÿ LLM 시스템의 출력에 대한 사실 검증 메커니즘 구축 Ÿ LLM 시스템의 출력에 의한 사용자 결정의 조작 방지 공정성과 포용성 인간중심, 공정성, 투명성 Ÿ LLM 시스템 출력에 유해한 편향이 없으며 개인이나 집단에 대한 불공정한  차별 부재 Ÿ LLM 시스템의 출력을 모든 최종 사용자가 이해 가능 고위험 사용 및  비의도적 사용 대처 인간중심, 안전성 Ÿ LLM 시스템이 본래 목적과 다르게 부적절하게 사용되어도 피해나 불이익  미발생  개인정보 보호 프라이버시 보호 Ÿ LLM 시스템이 정보의 중요성에 따라 프라이버시를 적절히 보호 보안  보안  Ÿ LLM 시스템의 허가되지 않은 운영 및 비의도적 수정 또는 중단으로 인한  기밀정보의 유출 방지 설명 가능성 투명성 Ÿ LLM 시스템 작동에 대한 증거 제시 등을 목적으로 출력의 근거를 기술적 으로 합리적인 범위에서 확인 가능 견고성 안전성, 투명성 Ÿ LLM 시스템이 적대적 프롬프트, 왜곡된 데이터 및 잘못된 입력 등 예상치  않은 입력에 대해 안정적 출력을 제공\n",
      "========================\n",
      "Document 5:\n",
      "2024년 11월호 Ⅰ. 인공지능 산업 동향 브리프  1. 정책/법제     ▹ 미국 민권위원회, 연방정부의 얼굴인식 기술 사용에 따른 민권 영향 분석 1    ▹ 미국 백악관 예산관리국, 정부의 책임 있는 AI 조달을 위한 지침 발표 2    ▹ 유로폴, 법 집행에서 AI의 이점과 과제를 다룬 보고서 발간 3    ▹ OECD, 공공 부문의 AI 도입을 위한 G7 툴킷 발표 4    ▹ 세계경제포럼, 생성AI 시대의 거버넌스 프레임워크 제시 5    2. 기업/산업     ▹ CB인사이츠 분석 결과, 2024년 3분기 벤처 투자 31%가 AI 스타트업에 집중 6    ▹ 메타, 동영상 생성AI 도구 ‘메타 무비 젠’ 공개 7    ▹ 메타, 이미지와 텍스트 처리하는 첫 멀티모달 AI 모델 ‘라마 3.2’ 공개 8    ▹ 앨런AI연구소, 벤치마크 평가에서 GPT-4o 능가하는 성능의 오픈소스 LLM ‘몰모’ 공개 9    ▹ 미스트랄AI, 온디바이스용 AI 모델 ‘레\n",
      "========================\n",
      "Document 6:\n",
      "1. 정책/법제   2. 기업/산업  3. 기술/연구   4. 인력/교육 5 세계경제포럼, 생성AI 시대의 거버넌스 프레임워크 제시 n 세계경제포럼이 글로벌 정책입안자를 대상으로 생성AI의 공익적 활용과 경제사회적  균형 달성, 위험 완화를 위한 거버넌스 프레임워크를 제안하는 백서를 발표 n 백서에 따르면 정부는 기존 규제를 평가해 생성AI로 인한 규제 격차를 해소하는 한편, 다양한  이해관계자 간 지식 공유를 촉진하고 미래의 AI 발전에 대비한 규제 민첩성을 갖출 필요 KEY Contents £ 생성AI 거버넌스, 과거-현재-미래를 아우르는 프레임워크 수립 필요 n 세계경제포럼(WEF)이 2024년 10월 8일 세계 각국의 정책입안자를 대상으로 생성AI 거버넌스  프레임워크를 제시한 백서를 발간 ∙백서는 생성AI의 공익적 활용과 경제사회적 균형 달성, 위험 완화라는 목표 달성을 위해 △과거  활용(Harness Past) △현재 구축(Build Present) △미래 계획(Plan Future)의 프레임워크를 제안 n (과거 활용) 기존\n",
      "========================\n",
      "메타는 최근 '메타 무비 젠(Meta Movie Gen)'이라는 동영상 생성 AI 도구를 공개했습니다. 이 도구는 텍스트 입력을 통해 고해상도 동영상을 생성할 수 있으며, 개인화된 동영상 제작, 동영상 편집, 오디오 생성 등의 기능을 지원합니다. 메타는 이 도구를 2025년 중 자사 플랫폼인 인스타그램에 통합할 계획입니다. 또한, 메타는 AI 모델의 성능을 높이기 위해 외부 파트너의 피드백을 반영하여 기능을 개선할 예정입니다.\n",
      "\n",
      "Final Response\n",
      "\n",
      "시스템: You are an expert AI on a question and answer task.   Use the \"Following Context\" when answering the question. If you don't know the answer, reply to the \"Following Text\" in the header and answer to the best of your knowledge, or if you do know the answer, answer without the \"Following Text\". If a question is asked in Korean, translate it to English and always answer in Korean.  Following Text: \"주어진 정보에서 답변을 찾지는 못했지만, 제가 아는 선에서 답을 말씀드려볼게요! **틀릴 수도 있으니 교차검증은 필수입니다!**\"  \n",
      "\n",
      "질문: 메타의 인공지능에 대해 알려줘 \n",
      "\n",
      "딥변: 메타는 최근 '메타 무비 젠(Meta Movie Gen)'이라는 동영상 생성 AI 도구를 공개했습니다. 이 도구는 텍스트 입력을 통해 고해상도 동영상을 생성할 수 있으며, 개인화된 동영상 제작, 동영상 편집, 오디오 생성 등의 기능을 지원합니다. 메타는 이 도구를 2025년 중 자사 플랫폼인 인스타그램에 통합할 계획입니다. 또한, 메타는 AI 모델의 성능을 높이기 위해 외부 파트너의 피드백을 반영하여 기능을 개선할 예정입니다. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    # query = input(\"질문을 입력하세요: \")\n",
    "    query = \"메타의 인공지능에 대해 알려줘\"\n",
    "    #print(\"========================\")\n",
    "    response = rag_chain_debug.invoke(query)\n",
    "    print()\n",
    "    print()\n",
    "    print(\"Final Response\")\n",
    "    print()\n",
    "    print(\"시스템:\", system_prompt,\"\\n\")\n",
    "    print(\"질문:\", query,\"\\n\")\n",
    "    print(\"딥변:\", response.content,\"\\n\")\n",
    "    save_output_to_file(query, response)# 결과를 /02_results/ 폴더에 저장\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9273d82-c010-4319-940a-8c151e8e7504",
   "metadata": {},
   "source": [
    "### 09_일반LLM(gpt-4o-mini)_답변"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "53d7aaa9-764c-4719-a919-e93de0650cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "메타(Meta, 이전의 페이스북)는 인공지능(AI) 기술을 다양한 분야에 활용하고 있습니다. 메타의 AI 연구는 주로 다음과 같은 영역에서 진행되고 있습니다:\n",
      "\n",
      "1. **자연어 처리(NLP)**: 메타는 자연어 처리 기술을 통해 사용자와의 상호작용을 개선하고, 콘텐츠 추천 시스템을 발전시키고 있습니다. 예를 들어, 메신저와 같은 플랫폼에서 사용자 간의 대화를 이해하고, 적절한 응답을 생성하는 데 AI를 활용합니다.\n",
      "\n",
      "2. **컴퓨터 비전**: 이미지와 비디오 분석을 위한 AI 기술도 개발하고 있습니다. 이는 사진 태그, 콘텐츠 필터링, 증강 현실(AR) 기능 등 다양한 응용 프로그램에 사용됩니다.\n",
      "\n",
      "3. **추천 시스템**: 메타는 사용자에게 맞춤형 콘텐츠를 제공하기 위해 AI 기반의 추천 알고리즘을 사용합니다. 이는 사용자 행동 데이터를 분석하여 개인화된 피드를 제공하는 데 기여합니다.\n",
      "\n",
      "4. **가상 현실(VR) 및 증강 현실(AR)**: 메타는 Oculus와 같은 VR 기기를 통해 몰입형 경험을 제공하며, AI를 활용하여 사용자 경험을 향상시키고 있습니다. 예를 들어, 가상 환경에서의 상호작용을 보다 자연스럽게 만드는 데 AI 기술이 사용됩니다.\n",
      "\n",
      "5. **안전 및 보안**: 메타는 AI를 사용하여 플랫폼에서의 부적절한 콘텐츠를 감지하고 차단하는 시스템을 개발하고 있습니다. 이는 사용자 안전을 보장하고 커뮤니티 가이드라인을 준수하는 데 중요한 역할을 합니다.\n",
      "\n",
      "메타는 AI 연구와 개발에 많은 자원을 투자하고 있으며, 이를 통해 사용자 경험을 개선하고, 새로운 기술을 선보이는 데 주력하고 있습니다. AI 기술의 발전은 메타의 비즈니스 모델과 서비스에 큰 영향을 미치고 있습니다.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "# OpenAI 클라이언트 설정\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "# 사용자 질문에 대한 completion 생성\n",
    "completion = client.chat.completions.create(\n",
    "    model='gpt-4o-mini',  # 사용하려는 모델\n",
    "    messages=[{'role': 'user', 'content': query}],\n",
    "    temperature=0.0  # 정확한 응답을 원할 때 사용\n",
    ")\n",
    "\n",
    "# 응답 출력\n",
    "print(completion.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cac2db8-cb4f-4d5a-93ab-3d87d26e0dd6",
   "metadata": {},
   "source": [
    "### 랭체인을 사용하는 이유: 최신 정보 반영의 중요성\n",
    "\n",
    "랭체인(Chain)은 **일반적인 AI 모델**보다 **실시간 데이터와 최신 정보를 반영**할 수 있다는 점에서 유리합니다. 이는 여러 가지 이유로 중요합니다:\n",
    "\n",
    "1. **최신 정보 반영**  \n",
    "   일반적인 AI 모델은 고정된 데이터셋을 기반으로 학습되기 때문에, **새로운 정보나 실시간 데이터를 반영하는 데 한계**가 있을 수 있습니다. 예를 들어, 최신 기술 발표나 업데이트된 벤치마크 결과 같은 정보는 AI 모델이 학습 후에는 반영되지 않거나 오래된 데이터일 수 있습니다.\n",
    "\n",
    "2. **랭체인의 실시간 데이터 처리**  \n",
    "   반면, **랭체인**은 **외부 데이터 소스와 실시간으로 연결**되어 정보를 즉시 가져오고 처리할 수 있습니다. 즉, 사용자가 요구하는 질문에 대해 **최신 뉴스**나 **업데이트된 정보를 실시간으로 반영**하여 보다 **정확하고 신뢰할 수 있는 답변**을 제공할 수 있습니다.\n",
    "\n",
    "3. **정확성 강화**  \n",
    "   예를 들어, **메타의 메타 무비 젠**과 같은 **최신 기술 발표**나 **최신 AI 모델 성능**에 대한 정보는 랭체인을 통해 즉시 반영됩니다. 이는 **일반 AI 모델**이 **학습하지 못한 최신 정보**를 제공함으로써, 보다 **정확하고 실시간적인 답변**을 가능하게 합니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p310",
   "language": "python",
   "name": "p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
