
(SLM?) - N-Gram - 워드임베딩(Word2Vec)
원핫인코딩



## Unigram이란?

- **N-그램(N-gram)**: N-그램은 연속된 N개의 단어로 구성된 그룹을 말해. 예를 들어, "나는 사과를 먹었다"라는 문장이 있을 때:
  - **1-그램 (Unigram)**: 각 단어를 개별적으로 보는 것. (예: "나는", "사과를", "먹었다")
  - **2-그램 (Bigram)**: 두 개의 단어를 묶어서 보는 것. (예: "나는 사과를", "사과를 먹었다")
  - **3-그램 (Trigram)**: 세 개의 단어를 묶어서 보는 것. (예: "나는 사과를 먹었다")

### n=1 (1-gram) 의 의미

Unigram에서 n=1이라는 것은 "각 단어 하나하나를 따로 고려한다"는 뜻, 문장을 구성하는 각 단어를 독립적으로 분석하고 확률을 계산하는 방식
n=3 (3-gram)의 경우 검색어 추천 사용에서는 효과적임


### biased

**편향된 상태 : 그룹이나 상황에 대해 불공정한 결정을 내리거나 결과를 만들어내는 것**


데이터 편향: AI가 훈련되는 데이터가 특정 그룹(예: 성별, 인종, 연령 등)에 대해 과대대표되거나 과소대표될 

알고리즘 편향: 알고리즘이 특정한 가정이나 오류를 기반으로 설계되면, 그 결과도 편향

사회적 편향: AI가 인간의 편견을 그대로 반영


### 원핫 인코딩(One-Hot Encoding)과 워드 인베딩(Word Embedding)

**원핫 인코딩 (One-Hot Encoding)**

정의: 원핫 인코딩은 각 단어를 고유한 벡터로 표현하는 방법, 벡터는 단어의 순서에 따라 특정 위치에 1을 두고 나머지 위치는 0

- "사과"
- "바나나"
- "체리"

- "사과": [1, 0, 0]
- "바나나": [0, 1, 0]
- "체리": [0, 0, 1]

장점: 구현이 간단하고 단어 간의 구분이 명확해.
단점: 단어의 의미나 관계를 반영하지 못하고, 벡터의 차원이 단어 수에 비례해서 커지는 문제가 있어.

### 2. 워드 인베딩 (Word Embedding)

워드 인베딩은 단어를 고차원 벡터로 변환, 단어 간의 의미적 관계를 반영하는 방법, 주로 신경망을 사용해 단어의 문맥을 학습하여 벡터를 생성.

예시*

- "사과": [0.1, 0.3, 0.5]
- "바나나": [0.2, 0.4, 0.6]
- "체리": [0.15, 0.35, 0.55]

이 벡터들은 단어 간의 유사성을 반영해. 예를 들어, "사과"와 "바나나"는 비슷한 과일이므로 이 두 벡터가 가까운 값을 가질 가능성이 높아.

장점: 단어 간의 의미적 관계를 잘 반영하고, 차원이 더 작아 효율적이야.
단점: 학습하는 데 시간이 더 걸리고, 구현이 복잡해.


### Sparse Vector

Sparse Vector는 대부분의 요소가 0인 벡터를 의미
일반적으로 큰 차원의 데이터에서 유용, 많은 값이 0이기 때문에 메모리와 높은 계산 효율성

희소성: 벡터의 많은 부분이 0으로 채워져 있어서, 실질적으로 중요한 값만 포함되어 있어.
저장 효율성: 0이 많은 데이터를 저장할 때, 메모리를 절약.
계산 효율성: 0을 무시하고 계산할 수 있기 때문에, 계산 속도가 빨라질 수 있어.

5차원 벡터 [0,0,3,0,4][0,0,3,0,4]이 있을 때.

이 벡터를 저장할 때, 다음과 같은 방식으로 표현

    인덱스와 값:
        (2, 3)
        (4, 4)

여기서 (인덱스, 값)은 실제로 저장된 위치와 그 값


### Dense Vector

모든 요소가 0이 아닐 가능성이 높은 벡터,  각 요소가 중요한 정보, 일반적으로 차원이 비교적 낮고, 값들이 연속적이거나 밀집

밀집성: 대부분의 요소가 0이 아니며, 각 요소가 중요한 의미를 가질 수 있어.
저장 방식: 모든 값을 그대로 저장하므로, 메모리 사용량이 상대적으로 많을 수 있어. 하지만 구조가 간단해 데이터 처리에 편리해.
계산 효율성: 모든 요소가 값이 있으므로, 벡터 간의 연산(예: 내적, 거리 계산)이 간단하고 빠르게 수행될 수 있어.

예시

5차원 벡터 [0.1,0.5,0.3,0.2,0.4][0.1,0.5,0.3,0.2,0.4] 

워드 임베딩(예: Word2Vec, GloVe)과 같은 모델에서 사용
각 단어를 고차원 공간의 밀집된 벡터로 표현해. 이 경우, 단어 간의 의미적 유사성을 잘 반영

이미지 처리나 신경망에서도 Dense vector를 많이 사용해, 데이터의 복잡한 패턴을 표현하고 학습하는 데 유용

### Word2Vec

단어를 벡터로 변환하는 기계 학습 모델
단어의 의미와 문맥을 반영한 밀집 벡터를 생성하는 데 사용돼
단어 간의 유사성을 잘 포착할 수 있도록 설계


Continuous Bag of Words (CBOW)
주어진 문맥(주변 단어들)을 바탕으로 중심 단어를 예측하는 방식
예를 들어, 문장 "나는 사과를 좋아한다"에서 "사과를"이라는 중심 단어를 예측하기 위해 "나는", "좋아한다"라는 주변 단어를 사용

Skip-gram
중심 단어를 바탕으로 주변 단어들을 예측하는 방식이야.
같은 예문에서 "사과를"을 중심 단어로 삼고, 주변 단어인 "나는", "좋아한다"를 예측해.

밀집 벡터: Word2Vec은 단어를 고차원 공간의 밀집 벡터로 변환하여, 각 단어 간의 의미적 관계를 반영
유사성: 벡터 공간에서 비슷한 의미를 가진 단어들은 가까운 위치에 배치, 예를 들어 "왕"과 "여왕", "남자"와 "여자" 같은 단어들이 유사하게 나타나.
수학적 연산 가능: 간단한 수학적 연산을 통해 의미적 관계를 표현, "여자" - "여왕" + "왕" = "남자"와 같은 계산이 가능


자연어 처리(NLP) 분야에서 광범위하게 사용

문서 분류: 단어를 벡터로 변환해 텍스트 분류 모델에 입력
추천 시스템: 사용자와 아이템 간의 유사성을 계산
질문 응답 시스템: 유사한 질문을 찾거나 답변을 생성하는 데 활용


### dot product

(내적 또는 점곱)은 두 벡터 간의 관계를 수치적으로 나타내는 연산
주로 기하학적 의미와 함께 벡터 간의 유사성을 평가하는 데 사용 
양수: 두 벡터가 비슷한 방향을 가질 때 (각도가 0도 이하).
음수: 두 벡터가 반대 방향을 가질 때 (각도가 90도 이상).
0: 두 벡터가 수직일 때 (각도가 90도).

유사도 측정: 텍스트 분석이나 추천 시스템에서 두 벡터(예: 단어 임베딩, 사용자 프로필 등) 간의 유사성을 측정할 때 사용해.
물리학: 힘과 이동의 관계를 계산할 때도 활용돼.

### backpropagation

(역전파)는 인공 신경망에서 가중치를 조정하기 위해 사용하는 알고리즘. 주로 딥러닝에서 모델을 훈련할 때 오류를 최소화하는 데 중요한 역할을 해. 이 과정은 다음과 같이 진행돼:

#### 순전파 (Forward Pass)

- 입력 데이터가 신경망을 통과하면서 각 레이어에서 계산이 이루어져. 이 과정에서 가중치와 활성화 함수를 사용해 출력값을 계산해.
- 예를 들어, 입력 \( X \)가 있을 때, 출력 \( Y \)는 \( Y = f(W \cdot X + b) \) 형태로 계산될 수 있어. 여기서 \( W \)는 가중치, \( b \)는 편향, \( f \)는 활성화 함수야.

#### 손실 계산

- 출력값 \( Y' \)와 실제 값 \( Y \) 간의 오차(손실)를 계산해. 손실 함수는 이 오차를 측정하는 데 사용되며, 일반적으로 Mean Squared Error (MSE) 또는 Cross-Entropy Loss가 사용돼.

#### 역전파 (Backpropagation)

오차 전파: 손실을 기반으로 각 가중치가 얼마나 기여했는지를 계산해. 이 과정에서 연쇄 법칙(Chain Rule)을 사용해 미분을 통해 가중치의 기울기를 구해.
  
\[
\frac{\partial L}{\partial W} = \frac{\partial L}{\partial Y'} \cdot \frac{\partial Y'}{\partial W}
\]

여기서 \( L \)은 손실 함수, \( Y' \)는 예측된 값이야.

가중치 업데이트 계산된 기울기를 사용해 가중치를 업데이트해. 일반적으로 경사 하강법(Gradient Descent) 방법을 사용해 다음과 같이 업데이트해:

\[
W_{\text{new}} = W_{\text{old}} - \eta \cdot \frac{\partial L}{\partial W}
\]

여기서 \( \eta \)는 학습률(learning rate)로, 가중치를 얼마나 업데이트할지를 결정해.

#### 반복

- 이 과정을 여러 번 반복하면서 신경망이 학습해. 각 에포크(epoch)마다 모든 데이터에 대해 순전파와 역전파를 수행하고 가중치를 조정해.

이미지 인식: CNN(합성곱 신경망)에서 이미지를 분류할 때.
자연어 처리: RNN(순환 신경망)에서 문장을 이해하고 생성할 때.
추천 시스템: 사용자와 아이템의 관계를 학습할 때.



### 경사 하강 (gradient Descent)

경사 하강법(Gradient Descent)은 머신러닝과 최적화에서 널리 사용되는 알고리즘으로, 목표 함수의 최소값을 찾기 위해 파라미터를 반복적으로 업데이트하는 방법

초기화: 파라미터(예: 가중치)를 무작위로 초기화
기울기 계산: 현재 파라미터 위치에서 목표 함수의 기울기를 계산
업데이트: 계산된 기울기를 사용해 파라미터를 업데이트
반복: 원하는 정확도에 도달하거나, 최대 반복 횟수에 도달할 때까지 2-3 단계를 반복

배치 경사 하강법(Batch Gradient Descent): 전체 데이터셋을 사용해 기울기를 계산합니다. 정확하지만, 큰 데이터셋에 대해 느림

확률적 경사 하강법(Stochastic Gradient Descent, SGD): 각 데이터 포인트를 하나씩 사용해 기울기를 계산합니다. 빠르지만, 노이즈가 있어 경로가 불안정

미니 배치 경사 하강법(Mini-batch Gradient Descent): 데이터셋을 작은 배치로 나누어 사용하는 방법으로, 배치와 SGD의 장점을 결합



좋아요! 필기하신 내용을 기반으로 각각의 개념을 자세히 설명해드릴게요.

### Gradient (기울기)
기울기(Gradient): 특정 점에서의 함수의 변화율을 나타내며, 방향과 크기를 가진 벡터입니다. 함수의 입력 변수들에 대한 기울기를 계산하면, 각 변수에 대해 함수가 얼마나 민감하게 반응하는지를 알 수 있습니다. 

순간 변화율/도함수: 미분의 개념과 관련이 있으며, 함수의 특정 점에서의 변화율을 나타냅니다. 기울기가 0인 지점은 함수의 최대값, 최소값, 혹은 최적점일 수 있습니다.

### Parameter (파라미터)
모델이 학습하는 과정에서 조정되는 값들. **가중치**를 의미. 가중치는 입력 데이터가 출력에 미치는 영향을 조절하는 역할을 합니다.

### Chain Rule (연쇄 법칙)
다변수 함수의 미분을 계산할 때 사용됩니다. 신경망과 같이 여러 레이어가 있는 경우, 각 레이어의 출력이 다음 레이어의 입력이 되므로, 전체 함수의 미분을 구하기 위해 각 레이어에서의 미분을 연쇄적으로 적용합니다. 이는 경사 하강법에서 기울기를 계산할 때 필수적입니다.

### 다양한 함수
지수 함수, 로그 함수, 삼각 함수: 머신러닝과 딥러닝에서는 다양한 함수 형태가 사용됩니다. 예를 들어, 활성화 함수로 지수 함수(예: ReLU), 로그 함수(예: 소프트맥스), 삼각 함수(예: 시그모이드)가 활용됩니다.

### 다변수 함수 (Multivariate Function)
여러 개의 입력 변수를 가지는 함수입니다. 예를 들어, \(f(x, y)\)는 두 개의 변수 \(x\)와 \(y\)를 가진 다변수 함수입니다. 머신러닝에서는 여러 특성(피처)을 고려하여 모델을 학습하므로 다변수 함수가 자주 사용됩니다.

### 상수 (Constant)
변하지 않는 값으로, 함수나 모델의 특정 값으로 고정된 수치를 의미합니다. 머신러닝에서는 파라미터가 아닌 고정된 값으로 사용되며, 일반적으로 편미분 계산에서 기울기를 구할 때에는 상수를 무시합니다.

### 다변수 벡터와 편미분
다변수 벡터: 세 개 이상의 변수를 포함하는 벡터입니다. 예를 들어, \(\mathbf{x} = [x_1, x_2, x_3]^T\)와 같이 나타낼 수 있습니다. 
편미분: 다변수 함수에서 한 변수를 고정하고 나머지 변수에 대해 미분하는 것입니다. 예를 들어, \(f(x, y)\)를 \(x\)에 대해 편미분하면 \( \frac{\partial f}{\partial x} \)가 됩니다.

### 변화량
0에 가까워졌을 때 \(y\)가 얼마나 변했는가**: 이는 미분의 기본 개념으로, 함수가 특정 점에서 얼마나 민감하게 반응하는지를 설명합니다. 즉, \(x\)가 0에 가까워질 때 \(y\)의 변화량을 이해하는 것은 함수의 기울기를 이해하는 데 도움이 됩니다.

이러한 개념들은 머신러닝 모델의 훈련 과정에서 중요한 역할을 하며, 모델이 데이터를 통해 학습하고 최적화를 수행하는 데 필요한 수학적 기초를 제공합니다.



































