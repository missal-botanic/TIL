
(SLM?) - N-Gram - 워드임베딩(Word2Vec)
원핫인코딩



## Unigram이란?

- **N-그램(N-gram)**: N-그램은 연속된 N개의 단어로 구성된 그룹을 말해. 예를 들어, "나는 사과를 먹었다"라는 문장이 있을 때:
  - **1-그램 (Unigram)**: 각 단어를 개별적으로 보는 것. (예: "나는", "사과를", "먹었다")
  - **2-그램 (Bigram)**: 두 개의 단어를 묶어서 보는 것. (예: "나는 사과를", "사과를 먹었다")
  - **3-그램 (Trigram)**: 세 개의 단어를 묶어서 보는 것. (예: "나는 사과를 먹었다")

### n=1 (1-gram) 의 의미

Unigram에서 n=1이라는 것은 "각 단어 하나하나를 따로 고려한다"는 뜻, 문장을 구성하는 각 단어를 독립적으로 분석하고 확률을 계산하는 방식
n=3 (3-gram)의 경우 검색어 추천 사용에서는 효과적임


### biased

**편향된 상태 : 그룹이나 상황에 대해 불공정한 결정을 내리거나 결과를 만들어내는 것**


데이터 편향: AI가 훈련되는 데이터가 특정 그룹(예: 성별, 인종, 연령 등)에 대해 과대대표되거나 과소대표될 

알고리즘 편향: 알고리즘이 특정한 가정이나 오류를 기반으로 설계되면, 그 결과도 편향

사회적 편향: AI가 인간의 편견을 그대로 반영


### 원핫 인코딩(One-Hot Encoding)과 워드 인베딩(Word Embedding)

**원핫 인코딩 (One-Hot Encoding)**

정의: 원핫 인코딩은 각 단어를 고유한 벡터로 표현하는 방법, 벡터는 단어의 순서에 따라 특정 위치에 1을 두고 나머지 위치는 0

- "사과"
- "바나나"
- "체리"

- "사과": [1, 0, 0]
- "바나나": [0, 1, 0]
- "체리": [0, 0, 1]

장점: 구현이 간단하고 단어 간의 구분이 명확해.
단점: 단어의 의미나 관계를 반영하지 못하고, 벡터의 차원이 단어 수에 비례해서 커지는 문제가 있어.

### 2. 워드 인베딩 (Word Embedding)

워드 인베딩은 단어를 고차원 벡터로 변환, 단어 간의 의미적 관계를 반영하는 방법, 주로 신경망을 사용해 단어의 문맥을 학습하여 벡터를 생성.

예시*

- "사과": [0.1, 0.3, 0.5]
- "바나나": [0.2, 0.4, 0.6]
- "체리": [0.15, 0.35, 0.55]

이 벡터들은 단어 간의 유사성을 반영해. 예를 들어, "사과"와 "바나나"는 비슷한 과일이므로 이 두 벡터가 가까운 값을 가질 가능성이 높아.

장점: 단어 간의 의미적 관계를 잘 반영하고, 차원이 더 작아 효율적이야.
단점: 학습하는 데 시간이 더 걸리고, 구현이 복잡해.


### Sparse Vector

Sparse Vector는 대부분의 요소가 0인 벡터를 의미
일반적으로 큰 차원의 데이터에서 유용, 많은 값이 0이기 때문에 메모리와 높은 계산 효율성

희소성: 벡터의 많은 부분이 0으로 채워져 있어서, 실질적으로 중요한 값만 포함되어 있어.
저장 효율성: 0이 많은 데이터를 저장할 때, 메모리를 절약.
계산 효율성: 0을 무시하고 계산할 수 있기 때문에, 계산 속도가 빨라질 수 있어.

5차원 벡터 [0,0,3,0,4][0,0,3,0,4]이 있을 때.

이 벡터를 저장할 때, 다음과 같은 방식으로 표현

    인덱스와 값:
        (2, 3)
        (4, 4)

여기서 (인덱스, 값)은 실제로 저장된 위치와 그 값


### Dense Vector

모든 요소가 0이 아닐 가능성이 높은 벡터,  각 요소가 중요한 정보, 일반적으로 차원이 비교적 낮고, 값들이 연속적이거나 밀집

밀집성: 대부분의 요소가 0이 아니며, 각 요소가 중요한 의미를 가질 수 있어.
저장 방식: 모든 값을 그대로 저장하므로, 메모리 사용량이 상대적으로 많을 수 있어. 하지만 구조가 간단해 데이터 처리에 편리해.
계산 효율성: 모든 요소가 값이 있으므로, 벡터 간의 연산(예: 내적, 거리 계산)이 간단하고 빠르게 수행될 수 있어.

예시

5차원 벡터 [0.1,0.5,0.3,0.2,0.4][0.1,0.5,0.3,0.2,0.4] 

워드 임베딩(예: Word2Vec, GloVe)과 같은 모델에서 사용
각 단어를 고차원 공간의 밀집된 벡터로 표현해. 이 경우, 단어 간의 의미적 유사성을 잘 반영

이미지 처리나 신경망에서도 Dense vector를 많이 사용해, 데이터의 복잡한 패턴을 표현하고 학습하는 데 유용

### Word2Vec

단어를 벡터로 변환하는 기계 학습 모델
단어의 의미와 문맥을 반영한 밀집 벡터를 생성하는 데 사용돼
단어 간의 유사성을 잘 포착할 수 있도록 설계


Continuous Bag of Words (CBOW)
주어진 문맥(주변 단어들)을 바탕으로 중심 단어를 예측하는 방식
예를 들어, 문장 "나는 사과를 좋아한다"에서 "사과를"이라는 중심 단어를 예측하기 위해 "나는", "좋아한다"라는 주변 단어를 사용

Skip-gram
중심 단어를 바탕으로 주변 단어들을 예측하는 방식이야.
같은 예문에서 "사과를"을 중심 단어로 삼고, 주변 단어인 "나는", "좋아한다"를 예측해.

밀집 벡터: Word2Vec은 단어를 고차원 공간의 밀집 벡터로 변환하여, 각 단어 간의 의미적 관계를 반영
유사성: 벡터 공간에서 비슷한 의미를 가진 단어들은 가까운 위치에 배치, 예를 들어 "왕"과 "여왕", "남자"와 "여자" 같은 단어들이 유사하게 나타나.
수학적 연산 가능: 간단한 수학적 연산을 통해 의미적 관계를 표현, "여자" - "여왕" + "왕" = "남자"와 같은 계산이 가능


자연어 처리(NLP) 분야에서 광범위하게 사용

문서 분류: 단어를 벡터로 변환해 텍스트 분류 모델에 입력
추천 시스템: 사용자와 아이템 간의 유사성을 계산
질문 응답 시스템: 유사한 질문을 찾거나 답변을 생성하는 데 활용


### dot product

(내적 또는 점곱)은 두 벡터 간의 관계를 수치적으로 나타내는 연산
주로 기하학적 의미와 함께 벡터 간의 유사성을 평가하는 데 사용 

두 벡터 \( \mathbf{A} = [a_1, a_2, \ldots, a_n] \)와 \( \mathbf{B} = [b_1, b_2, \ldots, b_n] \)의 내적은 다음과 같이 계산돼:

\[
\mathbf{A} \cdot \mathbf{B} = a_1 \times b_1 + a_2 \times b_2 + \ldots + a_n \times b_n
\]

### 예시

예를 들어, 두 벡터가 다음과 같다고 하자:

- \( \mathbf{A} = [2, 3, 4] \)
- \( \mathbf{B} = [1, 0, -1] \)

이 두 벡터의 내적은 다음과 같아:

\[
\mathbf{A} \cdot \mathbf{B} = (2 \times 1) + (3 \times 0) + (4 \times -1) = 2 + 0 - 4 = -2
\]

### 기하학적 의미

내적은 두 벡터 간의 각도와 관련이 있어. 두 벡터 \( \mathbf{A} \)와 \( \mathbf{B} \)의 내적은 다음과 같이 각도 \( \theta \)를 포함한 식으로 표현할 수 있어:

\[
\mathbf{A} \cdot \mathbf{B} = |\mathbf{A}| \cdot |\mathbf{B}| \cdot \cos(\theta)
\]

여기서 \( |\mathbf{A}| \)와 \( |\mathbf{B}| \)는 각 벡터의 크기야. 이 식을 통해 내적의 값이 양수인지, 음수인지, 혹은 0인지를 통해 두 벡터 간의 관계를 파악할 수 있어.

- **양수**: 두 벡터가 비슷한 방향을 가질 때 (각도가 0도 이하).
- **음수**: 두 벡터가 반대 방향을 가질 때 (각도가 90도 이상).
- **0**: 두 벡터가 수직일 때 (각도가 90도).

### 활용

- **유사도 측정**: 텍스트 분석이나 추천 시스템에서 두 벡터(예: 단어 임베딩, 사용자 프로필 등) 간의 유사성을 측정할 때 사용해.
- **물리학**: 힘과 이동의 관계를 계산할 때도 활용돼.

### 정리

Dot Product는 두 벡터 간의 관계를 수치적으로 나타내고, 기하학적으로도 의미가 있는 중요한 연산이야. 다양한 분야에서 널리 사용되고 있어!



backpropagation

**Backpropagation**(역전파)는 인공 신경망에서 가중치를 조정하기 위해 사용하는 알고리즘이야. 주로 딥러닝에서 모델을 훈련할 때 오류를 최소화하는 데 중요한 역할을 해. 이 과정은 다음과 같이 진행돼:

### 1. 순전파 (Forward Pass)

- 입력 데이터가 신경망을 통과하면서 각 레이어에서 계산이 이루어져. 이 과정에서 가중치와 활성화 함수를 사용해 출력값을 계산해.
- 예를 들어, 입력 \( X \)가 있을 때, 출력 \( Y \)는 \( Y = f(W \cdot X + b) \) 형태로 계산될 수 있어. 여기서 \( W \)는 가중치, \( b \)는 편향, \( f \)는 활성화 함수야.

### 2. 손실 계산

- 출력값 \( Y' \)와 실제 값 \( Y \) 간의 오차(손실)를 계산해. 손실 함수는 이 오차를 측정하는 데 사용되며, 일반적으로 Mean Squared Error (MSE) 또는 Cross-Entropy Loss가 사용돼.

### 3. 역전파 (Backpropagation)

- **오차 전파**: 손실을 기반으로 각 가중치가 얼마나 기여했는지를 계산해. 이 과정에서 연쇄 법칙(Chain Rule)을 사용해 미분을 통해 가중치의 기울기를 구해.
  
\[
\frac{\partial L}{\partial W} = \frac{\partial L}{\partial Y'} \cdot \frac{\partial Y'}{\partial W}
\]

여기서 \( L \)은 손실 함수, \( Y' \)는 예측된 값이야.

- **가중치 업데이트**: 계산된 기울기를 사용해 가중치를 업데이트해. 일반적으로 경사 하강법(Gradient Descent) 방법을 사용해 다음과 같이 업데이트해:

\[
W_{\text{new}} = W_{\text{old}} - \eta \cdot \frac{\partial L}{\partial W}
\]

여기서 \( \eta \)는 학습률(learning rate)로, 가중치를 얼마나 업데이트할지를 결정해.

### 4. 반복

- 이 과정을 여러 번 반복하면서 신경망이 학습해. 각 에포크(epoch)마다 모든 데이터에 대해 순전파와 역전파를 수행하고 가중치를 조정해.

### 활용

Backpropagation은 딥러닝에서 매우 중요한 알고리즘으로, 다음과 같은 분야에서 널리 사용돼:

- **이미지 인식**: CNN(합성곱 신경망)에서 이미지를 분류할 때.
- **자연어 처리**: RNN(순환 신경망)에서 문장을 이해하고 생성할 때.
- **추천 시스템**: 사용자와 아이템의 관계를 학습할 때.

### 정리

Backpropagation은 신경망의 가중치를 효율적으로 업데이트하는 방법으로, 신경망이 학습할 수 있도록 도와줘. 이 알고리즘 덕분에 깊은 신경망도 효과적으로 훈련할 수 있게 되었어!





경사 하강 (gradient Descent)

graident = 기울기 / 민감도 / 순간변화률 / 도함수 / 미분
최대 최소 최적점

parameter = 가중치

chain-rule = 레이어가 많아서

지수 함수/ 로그 함수 /삼각 함수

다변수 함수(multivariate funcion)
상수(constant)

다변수 백터(3개 이상?) -> 편미분

0에 가까워 졌을떄 y가 얼마나 변했는가




































