## 본 데이터의 특징을 나타는 도구

### 벡터

스칼라

시점, 종점

크기: 화살표 길이
방향: 화살표 방향 

인공지능의 벡터 = 위치벡터(벡터의 종점)

1. 길이 : 피타고라스 정의

2.방향은 코사인세타


1. 벡터의 내적(inner product . dot)(PCA) : 벡터가 다른 벡터에 기여하는 정도

2. 위치벡터 :  두 x 곱하고 y 곱 하서 더하면 된다. 0보다 크면 같은 반향, 0 이면 수직, 0보다 작으면 다른 방향

### 평균, 분산, 표준편차 

평균 수식을 사용한 표준편차 수식은 그 값이 0이 되는 것을 막기 위해 '제곱'을 실행함 -> 루트(제곱근 원위치)로 다시 내림
'본인의 값'에서 '평균'을 빼고 제곱한 갑들을 나눈다.

평균 + 표준편차, 평균 - 표준편차 = 하위 |-16%|-34%| 평균 |+34%|+16%| = 68%

### 표준화 (같은 기준 + 이상치 확인)

컴퓨터는 큰수만 인정, 점수 90/100 과 180/200이 다르게 해석 = 인공지능으 볼륨(Volume)만 본다. 

데이터를 0(평균), 표준편차(1)로 변환

정의는 Z




1. 평균(Mean)이 0

    편균은 데이터의 평균을 의미합니다. 편균이 0이라는 것은 데이터의 값들이 평균적으로 0에 중심을 두고 있다는 것을 의미합니다.
    예를 들어, 데이터가 -1, 0, 1이라면 이 세 수의 평균은 0입니다.

2. 표준편차(Standard Deviation)가 1

    표준편차는 데이터가 평균에서 얼마나 퍼져 있는지를 나타내는 지표입니다. 표준편차가 1이라는 것은 데이터가 평균(0)에서 평균적으로 1 단위 정도 떨어져 있다는 의미입니다.
    즉, 대부분의 데이터가 -1에서 1 사이에 분포할 가능성이 높습니다.

정리

    편균이 0이고 표준편차가 1인 데이터는 정규분포의 표준정규분포를 나타냅니다. 이 경우, 분포의 모양은 종 모양을 하고 있으며, 68%의 데이터가 평균 ±1 표준편차 내에 존재하고, 95%는 ±2 표준편차 내에 존재합니다.




 ## 두데이터의 관계 분석

 ### 공분산 (Corariance)   

 공분산 둘이 하나 늘때 하나 같이 늘어난다.
 
공분산이 음수가 하나가 될 때 하나 감소

선에 붙을수록 공분산이 크다. 

하나가 늘때 하나가 더 확실히 는다.

하나가 늘때 하나가 확 감소 한다.

### 표준화 -> 상관 계수

음의 상관 관계-1 < r <0 [\]
독립 서로 영향을 주지 않는다 r = 0 [o]
양의 상관 관계 0<r<1 [/]

0.8 높다 , 0.6 일반, 0~0.2 관계 없다.[1이 나올경우 잘못될 가능성 크다]

## 선형회기(단순회기) 두 데이터를 관계 뿐만 아니라 더 정확한 수식(방적식)을 알고 싶다.

'에러'를 제거한 두 데이터 간의 관계 도출

선형회기 유도(기울기)


[1,2,3]=x [1,3,4]=y

1을 넣으면 1이 나와야 하고
2을 넣으면 3이 나와야하고
3을 넣으면 4가 나오는 수식


## 행렬(matrix)

2x2 행렬
[1 2]
[3 4]

###전치 행렬(transpose)

위치 변경
[1 2]       [1 3]
[3 4]  ->   [2 4]

전치 행렬의 활용

    기하학: 2D 또는 3D 공간에서의 벡터 변환에 사용됩니다.
    선형 대수학: 행렬 연산, 특히 곱셈과 관련된 성질을 증명하는 데 유용합니다.
    데이터 처리: 데이터 프레임을 다룰 때, 행과 열을 전환하는 작업에 사용됩니다.

###행렬의 곱
곱은 앞열 뒤행이 형태가 같아야 한다.

기하학적 의미 = 공간으로 그리면 왜 그렇게 되는지 시각화

mapping = 행렬 곱하는것

- 2x2 * 2x2 에서 2x[2 * 2]x2 가운데 [*]부분이 사라진다.
- 2x3 * 3x2 에서 가운데 [3 * 3]이 사라져서 2x2 행렬이 된다.



[1,2] [1,a]
      [b,1]
a = x를 y값 만큼 추가
b = y를 x값 만큰 추가

딥러닝, PCA 분석에 쓰임

1)point(vector과 다르다) True or False(0.1)
2)Scalar(수직선 위) 1.2.3.4 (차트의 대가선 수직선도 포함)
3)Vector(xyz역시 2차원 정보) [1.2.3].[5.6.4.1]
4)matrix [1.2.3]
       [2.4.6]

Tensor 400x300 이미지 한 픽셀에 rgb x3 까지 더한것

4)차원의 증가



## 선형독립

### Linear Independent(서로 다른 두 벡터가 얼마나 다른 개성을 가지고 있는가?)
### Linear Dependent(종속)

A 방향으로 이동하고 B 방향을로 그리드를 만들 수 있을 때 선형 독릭이다. 

조합을 통해서 다른 점으로 이동가능한가.

벡터의 덧셈 = a 백터 이동후 그 지점이 시작점이 되에 b 벡터 진행 = b 백터가 평행 이동
그러면 c는 a 시작점에서 b의 종착점까지의 선

Basis : R + 벡터 n개가 나오는 것

Span : R 공간을 linear Landependent화 하는 것 


[2 1]
[1 2]
 a b

a[2.1] + b[1.2] = 열로 대입한다(행이 아니다)
곱할때 열로 가는 이유


## Matrix as Vector Space, Rank
Rank = 차원
2차원에 1 Rank = 같은선에 2개의 점
3차원에 2 Rank(평면만 가능) 
[1 2 0]
[2 4 0]
[0 0 1]

3차원에 1 Rank(선만 가능) 
[1 2 3]
[0 0 0]
[0 0 0]

Matrix R rxr 의 Rank 가 n = 랭크가 보존된다. = linear Landependent(선형 독립)

a를 교정했을 때 b가 되면 안된다. 

Null Space, Ax=0

- Matrix X Vector = Vector

- Range Space + Null Space

Ax = 0 : 2개의 점이 종속일 경우 1개의  Null Space 를 곱해주고 중복된 2개중 1개만 남긴다.

### Matrix as Linear Operator

[1]    [3]
[1] -> [3]
원래세계 기준으로 새좌표 기준으로 보면 [1 1]은 [3 3]이다
행렬의 곱은 기준 좌표를 바꾼다. 1.1의 기준이 3.3으로 바뀐다.

[1 2]   []   []
[1 2] x [] = []
     새기준  = 기존기준에서 본 새기준


좌표 공간을 변형하고 업애고 줄여서 의미 있는 차원만 남겨서 w와 b를 찾는것


