## 본 데이터의 특징을 나타는 도구

### 벡터

스칼라

시점, 종점

크기: 화살표 길이
방향: 화살표 방향 

인공지능의 벡터 = 위치벡터(벡터의 종점)

1. 길이 : 피타고라스 정의

2.방향은 코사인세타


1. 벡터의 내적(inner product . dot)(PCA) : 벡터가 다른 벡터에 기여하는 정도

2. 위치벡터 :  두 x 곱하고 y 곱 하서 더하면 된다. 0보다 크면 같은 반향, 0 이면 수직, 0보다 작으면 다른 방향

### 평균, 분산, 표준편차 

평균 수식을 사용한 표준편차 수식은 그 값이 0이 되는 것을 막기 위해 '제곱'을 실행함 -> 루트(제곱근 원위치)로 다시 내림
'본인의 값'에서 '평균'을 빼고 제곱한 갑들을 나눈다.

평균 + 표준편차, 평균 - 표준편차 = 하위 |-16%|-34%| 평균 |+34%|+16%| = 68%

### 표준화 (같은 기준 + 이상치 확인)

컴퓨터는 큰수만 인정, 점수 90/100 과 180/200이 다르게 해석 = 인공지능으 볼륨(Volume)만 본다. 

데이터를 0(평균), 표준편차(1)로 변환

정의는 Z




1. 평균(Mean)이 0

    편균은 데이터의 평균을 의미합니다. 편균이 0이라는 것은 데이터의 값들이 평균적으로 0에 중심을 두고 있다는 것을 의미합니다.
    예를 들어, 데이터가 -1, 0, 1이라면 이 세 수의 평균은 0입니다.

2. 표준편차(Standard Deviation)가 1

    표준편차는 데이터가 평균에서 얼마나 퍼져 있는지를 나타내는 지표입니다. 표준편차가 1이라는 것은 데이터가 평균(0)에서 평균적으로 1 단위 정도 떨어져 있다는 의미입니다.
    즉, 대부분의 데이터가 -1에서 1 사이에 분포할 가능성이 높습니다.

정리

    편균이 0이고 표준편차가 1인 데이터는 정규분포의 표준정규분포를 나타냅니다. 이 경우, 분포의 모양은 종 모양을 하고 있으며, 68%의 데이터가 평균 ±1 표준편차 내에 존재하고, 95%는 ±2 표준편차 내에 존재합니다.




 ## 두데이터의 관계 분석

 ### 공분산 (Corariance)   

 공분산 둘이 하나 늘때 하나 같이 늘어난다.
 
공분산이 음수가 하나가 될 때 하나 감소

선에 붙을수록 공분산이 크다. 

하나가 늘때 하나가 더 확실히 는다.

하나가 늘때 하나가 확 감소 한다.

### 표준화 -> 상관 계수

음의 상관 관계-1 < r <0 [\]
독립 서로 영향을 주지 않는다 r = 0 [o]
양의 상관 관계 0<r<1 [/]

0.8 높다 , 0.6 일반, 0~0.2 관계 없다.[1이 나올경우 잘못될 가능성 크다]

## 선형회기(단순회기) 두 데이터를 관계 뿐만 아니라 더 정확한 수식(방적식)을 알고 싶다.

'에러'를 제거한 두 데이터 간의 관계 도출

선형회기 유도(기울기)


[1,2,3]=x [1,3,4]=y

1을 넣으면 1이 나와야 하고
2을 넣으면 3이 나와야하고
3을 넣으면 4가 나오는 수식


## 행렬(matrix)

2x2 행렬
[1 2]
[3 4]

###전치 행렬(transpose)

위치 변경
[1 2]       [1 3]
[3 4]  ->   [2 4]

전치 행렬의 활용

    기하학: 2D 또는 3D 공간에서의 벡터 변환에 사용됩니다.
    선형 대수학: 행렬 연산, 특히 곱셈과 관련된 성질을 증명하는 데 유용합니다.
    데이터 처리: 데이터 프레임을 다룰 때, 행과 열을 전환하는 작업에 사용됩니다.

###행렬의 곱
곱은 앞열 뒤행이 형태가 같아야 한다.

기하학적 의미 = 공간으로 그리면 왜 그렇게 되는지 시각화

mapping = 행렬 곱하는것

- 2x2 * 2x2 에서 2x[2 * 2]x2 가운데 [*]부분이 사라진다.
- 2x3 * 3x2 에서 가운데 [3 * 3]이 사라져서 2x2 행렬이 된다.



[1,2] [1,a]
      [b,1]
a = x를 y값 만큼 추가
b = y를 x값 만큰 추가

딥러닝, PCA 분석에 쓰임

1)point(vector과 다르다) True or False(0.1)
2)Scalar(수직선 위) 1.2.3.4 (차트의 대가선 수직선도 포함)
3)Vector(xyz역시 2차원 정보) [1.2.3].[5.6.4.1]
4)matrix [1.2.3]
       [2.4.6]

Tensor 400x300 이미지 한 픽셀에 rgb x3 까지 더한것

4)차원의 증가



## 선형독립

### Linear Independent(서로 다른 두 벡터가 얼마나 다른 개성을 가지고 있는가?)
### Linear Dependent(종속)

A 방향으로 이동하고 B 방향을로 그리드를 만들 수 있을 때 선형 독릭이다. 

조합을 통해서 다른 점으로 이동가능한가.

벡터의 덧셈 = a 백터 이동후 그 지점이 시작점이 되에 b 벡터 진행 = b 백터가 평행 이동
그러면 c는 a 시작점에서 b의 종착점까지의 선

Basis : R + 벡터 n개가 나오는 것

Span : R 공간을 linear Landependent화 하는 것 


[2 1]
[1 2]
 a b

a[2.1] + b[1.2] = 열로 대입한다(행이 아니다)
곱할때 열로 가는 이유


## Matrix as Vector Space, Rank
Rank = 차원
2차원에 1 Rank = 같은선에 2개의 점
3차원에 2 Rank(평면만 가능) 
[1 2 0]
[2 4 0]
[0 0 1]

3차원에 1 Rank(선만 가능) 
[1 2 3]
[0 0 0]
[0 0 0]

Matrix R rxr 의 Rank 가 n = 랭크가 보존된다. = linear Landependent(선형 독립)

a를 교정했을 때 b가 되면 안된다. 

Null Space, Ax=0

- Matrix X Vector = Vector

- Range Space + Null Space

Ax = 0 : 2개의 점이 종속일 경우 1개의  Null Space 를 곱해주고 중복된 2개중 1개만 남긴다.

### Matrix as Linear Operator

[1]    [3]
[1] -> [3]
원래세계 기준으로 새좌표 기준으로 보면 [1 1]은 [3 3]이다
행렬의 곱은 기준 좌표를 바꾼다. 1.1의 기준이 3.3으로 바뀐다.

[1 2]   []   []
[1 2] x [] = []
     새기준  = 기존기준에서 본 새기준


좌표 공간을 변형하고 업애고 줄여서 의미 있는 차원만 남겨서 w와 b를 찾는것 = 제곱처럼 값들의 특징을 더욱 잘 표현 [0] -> [/]

### 단위 행렬

곱해서 자기 자신이 되는 행렬
곱셉의 1, 덧셈의 0

### 역행렬

무엇인가 곱했을때 단위행렬이 되는것
원래 값으로 돌아가게 하는것

### 공분산 행렬
 정보가 포함된 새로운 좌표
 자신의 고유값이 아닌 정보가 표현된 새로운 좌표

 공분산이 양수이면 두 변수가 함께 증가하거나 감소하는 경향이 있음을 의미합니다. 음수일 경우 한 변수가 증가할 때 다른 변수가 감소하는 경향이 있음을 나타냅니다.

 변수 간 상관관계: 공분산 행렬을 통해 변수들이 어떻게 상호작용하는지를 이해할 수 있습니다.
주성분 분석(PCA): 공분산 행렬은 PCA와 같은 차원 축소 기법에서 중요한 역할을 합니다. PCA는 데이터의 분산이 가장 큰 방향을 찾는 방법인데, 이때 공분산 행렬의 고유값 분해를 사용합니다.
다변량 정규 분포: 공분산 행렬은 다변량 정규 분포의 모양을 정의합니다. 각 변수의 분포와 변수 간의 상관관계를 동시에 고려합니다.


 x=[1,2,3] y=[2,3,1] 1.2 / 2.3 / 3.1 의 데이터


### 고유벡터(eigen vector) 고유값(eigen value)

고유벡터 : 변환되어도 방향이 유지 되는 벡터
고유값 : 고유벡타가 늘어난 비율

고유벡터 + 공분산 행렬 = y가 가장 잘 퍼져있게 x가 가장 덜 퍼진 방향



### PCA 주성분분석

주성분(Principal Component)
1st PC
2nd PC (주 축의 수직)
...

분포된 포인트가 한개의 선에 가장 까까운 방식으로 붙음 -> 1차원으로 줄어듬 -> 과적합 문제 해결함
PC score = Weight(가중치)












return_tensors='pt'

    설명: 이 옵션은 반환할 텐서의 형식을 지정합니다.
        'pt': PyTorch 텐서를 반환합니다. 즉, 생성된 텐서는 PyTorch의 Tensor 형식으로 변환됩니다. PyTorch는 많은 머신러닝과 딥러닝 작업에서 널리 사용되는 프레임워크입니다.
        만약 TensorFlow를 사용하고 있다면, 'tf'를 사용할 수 있습니다.



차원의 축소가 과적합(overfitting)을 방지하는 이유는 여러 가지가 있습니다. 차원 축소는 데이터의 복잡성을 줄이고, 모델이 학습하는 데 필요한 정보만 남기기 때문에 일반화 성능을 개선하는 데 도움이 됩니다. 아래에 주요 이유를 설명하겠습니다.
1. 노이즈 감소

    데이터의 차원이 높을수록 노이즈(불필요한 정보)도 증가하는 경향이 있습니다. 차원 축소를 통해 중요한 특성만 남기고 노이즈를 제거하면 모델이 불필요한 정보에 적합하지 않도록 할 수 있습니다.

2. 해석 가능성

    차원 축소는 데이터의 구조를 단순화하여 시각화나 해석을 용이하게 만듭니다. 이를 통해 모델이 학습해야 할 패턴이 더 명확해져, 과적합의 가능성을 줄일 수 있습니다.

3. 모델의 복잡도 감소

    고차원 데이터에서는 더 많은 매개변수(파라미터)가 필요하게 되어 모델이 복잡해집니다. 복잡한 모델은 훈련 데이터에 과도하게 적합할 위험이 있습니다. 차원 축소를 통해 입력 변수의 수를 줄이면, 모델이 학습하는 데 필요한 파라미터 수도 줄어들어 과적합을 방지할 수 있습니다.

4. 정보의 압축

    차원 축소 기법(예: 주성분 분석, t-SNE 등)은 데이터를 요약하는 방법입니다. 주요 정보를 유지하면서 불필요한 특성을 제거하므로, 모델이 학습하는 데 필요한 정보만 남게 됩니다. 이로 인해 모델의 일반화 능력이 향상됩니다.

5. 데이터 샘플 수와의 관계

    고차원 공간에서는 데이터 샘플 간의 거리가 멀어지는 경향이 있어, 데이터가 희소해집니다. 이는 모델이 특정 샘플에 과적합하는 위험을 높입니다. 차원 축소를 통해 샘플 간의 밀도를 높이면 이러한 문제를 완화할 수 있습니다.

6. 특성 선택

    차원 축소 과정에서 중요하지 않은 특성이 제거되면서, 모델은 더 유의미한 특성에 집중하게 됩니다. 이는 불필요한 변동성을 줄이고, 더 강력한 예측 모델을 만드는 데 기여합니다.

요약

차원 축소는 데이터의 복잡성을 줄이고, 노이즈를 제거하며, 모델의 일반화 능력을 향상시키는 중요한 기법입니다. 이를 통해 과적합의 위험을 줄이고, 보다 효과적인 모델을 구축할 수 있습니다.






### 연속적인 데이터 상관계수

경향성 볼 수 있다.

불연속, 비연관 데이터

두 벡터의 각도가 세타값으로 알수 있다. 각도가 작을 수록 경향이 유사하다.

코사인 유사도로 유사도 선정 

- 같은 방향 0~1사이

- 수직은 0이다

- 반대 반향이면 -1까지 작아진다.

- 상관계수랑 같다

### 퍼셉트론

인간의 신경을 모델링함
노드와 로지스틱스 회귀의 기초

x1  w1  ->
x2  w2  -> w1x1 + w2x2 + w3b -> a
b(상식값)->

웨이트, 가중치 PC가 자동으로 찾아줌

### 다중 퍼셉트론

x1  w1  -> w1x1 + w2x2 + w3b 
x2  w2  ->                    -> a
b(상식값)-> w4x1 + w5x2 + w6b 

퍼셉트론 2개 써서 축을 바꿔서 나눌 수 있게 함


### 로그와 크로스피

2의 3제곱

음수의 제곱은 역수의 제곱

자연대수 e5 = 2.71

### 로그 함수

y=ex의 역함수

y=x 에 대한 대칭

### 정보 엔트로피


로그는 손실함수

값은 0~1사이에 위치

MSE : (예측 - 실제)2 평균 [로그 미사용]
CEE(cross entropy error) : 오차 0 과 오차 무한대  [로그 사용으로 극단적 결과로 수정 쉬움]

### 미분과 경사하강법

평균기울기 : 기울기의 평균

순간기울기 : 순간의 기울기 = x가 아주 미묘하게 변했을 때 y가 얼마나 변했는지 = 미분(아주 잘게 분리했다) 

미분에서 순간기울기 0인 지점 = 최대 최소 최적점 구하기

#### 경사하강법 (Gradient Desent) 
1.어떤점에서 기울기 구한다.
2.기울기 반대 방향으로 lr만큼 이동한다.(기울기 x lr)
3.기울기 0이 될 때까지 멈춘다.


학습률 lr: 하이퍼 파라메터

1. 안장점 문제 (Saddling Point) : 더 갈 수 있는데 0일때 멈춤
2. 느린 학습

-> 해결하기 위해 옵티마이져 기법 등장

%%% 경사하강법 (Gradient Desent) 에서의 lr 역할, 단점 극복 위해 옵티마이져 등장 
