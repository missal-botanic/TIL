

원핫인코딩 - N-Gram - 워드임베딩(Word2Vec)

### Unigram이란?

- **N-그램(N-gram)**: N-그램은 연속된 N개의 단어로 구성된 그룹을 말해. 예를 들어, "나는 사과를 먹었다"라는 문장이 있을 때:
  - **1-그램 (Unigram)**: 각 단어를 개별적으로 보는 것. (예: "나는", "사과를", "먹었다")
  - **2-그램 (Bigram)**: 두 개의 단어를 묶어서 보는 것. (예: "나는 사과를", "사과를 먹었다")
  - **3-그램 (Trigram)**: 세 개의 단어를 묶어서 보는 것. (예: "나는 사과를 먹었다")

### n=1 (1-gram) 의 의미

Unigram에서 n=1이라는 것은 "각 단어 하나하나를 따로 고려한다"는 뜻이야. 즉, 문장을 구성하는 각 단어를 독립적으로 분석하고 확률을 계산하는 방식이지.

### n=3 (3-gram)의 경우 검색어 추천 사용에서는 효과적임



### 예시

문장 "나는 사과를 먹었다"를 Unigram으로 나누면:

- "나는"
- "사과를"
- "먹었다"

각각의 단어에 대한 확률을 계산할 수 있어. 예를 들어, "나는"이라는 단어가 전체 단어 시퀀스에서 얼마나 자주 등장하는지를 확인할 수 있지.

이렇게 Unigram 모델은 단어들 간의 관계를 고려하지 않고, 단어를 독립적으로 다루기 때문에 구현이 간단하지만, 문맥을 반영하기에는 한계가 있어.


인공지능에서 "biased"는 편향된 상태를 의미해. 이는 AI 모델이나 알고리즘이 특정한 정보나 데이터에 의해서 영향을 받아, 특정 그룹이나 상황에 대해 불공정한 결정을 내리거나 결과를 만들어내는 것을 말해.
예를 들어:

    데이터 편향: AI가 훈련되는 데이터가 특정 그룹(예: 성별, 인종, 연령 등)에 대해 과대대표되거나 과소대표될 때 발생해. 이럴 경우 AI는 특정 그룹에 대해 부정확하거나 편향된 결과를 낼 수 있어.

    알고리즘 편향: 알고리즘이 특정한 가정이나 오류를 기반으로 설계되면, 그 결과도 편향될 수 있어. 예를 들어, 어떤 특정 조건에서만 잘 작동하도록 설계된 알고리즘은 다른 조건에서는 잘 작동하지 않을 수 있어.

    사회적 편향: AI가 인간의 편견을 그대로 반영할 수 있어. 예를 들어, 과거의 고정관념이나 차별적인 관행이 포함된 데이터로 훈련되면, AI는 그런 편견을 그대로 답습할 수 있어.



    원핫 인코딩(One-Hot Encoding)과 워드 인베딩(Word Embedding)은 자연어 처리에서 단어를 숫자로 표현하는 두 가지 방법이야. 각각의 특징을 예시와 함께 설명해줄게.





### 1. 원핫 인코딩 (One-Hot Encoding)

**정의**: 원핫 인코딩은 각 단어를 고유한 벡터로 표현하는 방법이야. 이 벡터는 단어의 순서에 따라 특정 위치에 1을 두고 나머지 위치는 0으로 설정돼.

**예시**:
단어 집합이 다음과 같다고 해보자:
- "사과"
- "바나나"
- "체리"

이 경우, 각 단어를 원핫 인코딩으로 표현하면:

- "사과": [1, 0, 0]
- "바나나": [0, 1, 0]
- "체리": [0, 0, 1]

**장점**: 구현이 간단하고 단어 간의 구분이 명확해.
**단점**: 단어의 의미나 관계를 반영하지 못하고, 벡터의 차원이 단어 수에 비례해서 커지는 문제가 있어.

### 2. 워드 인베딩 (Word Embedding)

**정의**: 워드 인베딩은 단어를 고차원 벡터로 변환해, 단어 간의 의미적 관계를 반영하는 방법이야. 주로 신경망을 사용해 단어의 문맥을 학습하여 벡터를 생성해.

**예시**:
"사과", "바나나", "체리"의 워드 인베딩 결과는 다음과 같을 수 있어 (실제 값은 훈련에 따라 달라짐):

- "사과": [0.1, 0.3, 0.5]
- "바나나": [0.2, 0.4, 0.6]
- "체리": [0.15, 0.35, 0.55]

이 벡터들은 단어 간의 유사성을 반영해. 예를 들어, "사과"와 "바나나"는 비슷한 과일이므로 이 두 벡터가 가까운 값을 가질 가능성이 높아.

**장점**: 단어 간의 의미적 관계를 잘 반영하고, 차원이 더 작아 효율적이야.
**단점**: 학습하는 데 시간이 더 걸리고, 구현이 복잡해.

### 정리

- **원핫 인코딩**: 단어를 고유한 벡터로 구분, 의미 관계 없음, 차원 큼.
- **워드 인베딩**: 단어의 의미적 관계를 반영한 벡터, 차원 작고 효율적.

이런 차이를 통해 각 방법이 어떤 상황에 적합한지 이해할 수 있어!



Sparse Vector는 대부분의 요소가 0인 벡터를 의미해. 일반적으로 큰 차원의 데이터에서 유용하게 사용되며, 많은 값이 0이기 때문에 메모리와 계산 효율성을 높일 수 있어.
특징

    희소성: 벡터의 많은 부분이 0으로 채워져 있어서, 실질적으로 중요한 값만 포함되어 있어.
    저장 효율성: 0이 많은 데이터를 저장할 때, 메모리를 절약할 수 있어. 예를 들어, 1000차원의 벡터에서 998개가 0이고 2개만 비어있다면, 모든 값을 저장하는 대신 비어 있지 않은 값만 저장할 수 있어.
    계산 효율성: 0을 무시하고 계산할 수 있기 때문에, 계산 속도가 빨라질 수 있어.

예시

예를 들어, 5차원 벡터 [0,0,3,0,4][0,0,3,0,4]이 있을 때, 이는 sparse vector로 간주될 수 있어. 대부분의 요소가 0이기 때문이지.

이 벡터를 저장할 때, 다음과 같은 방식으로 표현할 수 있어:

    인덱스와 값:
        (2, 3)
        (4, 4)

여기서 (인덱스, 값)은 실제로 저장된 위치와 그 값을 나타내. 이런 방식으로 저장하면 메모리를 절약할 수 있어.
사용 예

Sparse vector는 텍스트 데이터(예: 단어 빈도)나 추천 시스템에서 사용자와 아이템 간의 상호작용을 표현할 때 자주 사용돼. 예를 들어, 사용자가 특정 아이템에 대해 평가한 경우, 대부분의 아이템은 평가가 없을 수 있으므로, sparse vector를 사용하면 효율적으로 데이터를 표현하고 처리할 수 있어.


Dense Vector는 모든 요소가 0이 아닐 가능성이 높은 벡터를 의미해. 즉, 대부분의 값이 비어 있지 않아서, 각 요소가 중요한 정보를 담고 있어. 일반적으로 차원이 비교적 낮고, 값들이 연속적이거나 밀집되어 있는 경우에 사용돼.
특징

    밀집성: 대부분의 요소가 0이 아니며, 각 요소가 중요한 의미를 가질 수 있어.
    저장 방식: 모든 값을 그대로 저장하므로, 메모리 사용량이 상대적으로 많을 수 있어. 하지만 구조가 간단해 데이터 처리에 편리해.
    계산 효율성: 모든 요소가 값이 있으므로, 벡터 간의 연산(예: 내적, 거리 계산)이 간단하고 빠르게 수행될 수 있어.

예시

예를 들어, 5차원 벡터 [0.1,0.5,0.3,0.2,0.4][0.1,0.5,0.3,0.2,0.4]는 dense vector야. 이 벡터의 모든 요소가 0이 아니고, 각 값이 의미 있는 정보를 담고 있어.
사용 예

Dense vector는 주로 워드 임베딩(예: Word2Vec, GloVe)과 같은 모델에서 사용되며, 각 단어를 고차원 공간의 밀집된 벡터로 표현해. 이 경우, 단어 간의 의미적 유사성을 잘 반영할 수 있어.

또한, 이미지 처리나 신경망에서도 Dense vector를 많이 사용해, 데이터의 복잡한 패턴을 표현하고 학습하는 데 유용해.
정리

    Dense Vector: 대부분의 요소가 0이 아닌 밀집된 벡터. 정보를 풍부하게 담고 있으며, 계산이 간단하고 빠름.
    Sparse Vector: 대부분의 요소가 0인 희소한 벡터. 메모리와 계산 효율성이 높음.

이런 차이를 이해하면 데이터 처리와 기계 학습에서 두 벡터의 활용을 쉽게 구분할 수 있어!



**Word2Vec**는 단어를 벡터로 변환하는 기계 학습 모델로, 단어의 의미와 문맥을 반영한 밀집 벡터를 생성하는 데 사용돼. 이 모델은 단어 간의 유사성을 잘 포착할 수 있도록 설계되었어. 

### 주요 개념

Word2Vec은 두 가지 주요 모델을 기반으로 해:

1. **Continuous Bag of Words (CBOW)**:
   - 주어진 문맥(주변 단어들)을 바탕으로 중심 단어를 예측하는 방식이야.
   - 예를 들어, 문장 "나는 사과를 좋아한다"에서 "사과를"이라는 중심 단어를 예측하기 위해 "나는", "좋아한다"라는 주변 단어를 사용해.

2. **Skip-gram**:
   - 중심 단어를 바탕으로 주변 단어들을 예측하는 방식이야.
   - 같은 예문에서 "사과를"을 중심 단어로 삼고, 주변 단어인 "나는", "좋아한다"를 예측해.

### 특징

- **밀집 벡터**: Word2Vec은 단어를 고차원 공간의 밀집 벡터로 변환하여, 각 단어 간의 의미적 관계를 반영해.
- **유사성**: 벡터 공간에서 비슷한 의미를 가진 단어들은 가까운 위치에 배치돼, 예를 들어 "왕"과 "여왕", "남자"와 "여자" 같은 단어들이 유사하게 나타나.
- **수학적 연산 가능**: Word2Vec 벡터는 간단한 수학적 연산을 통해 의미적 관계를 표현할 수 있어. 예를 들어, "여자" - "여왕" + "왕" = "남자"와 같은 계산이 가능해.

### 활용

Word2Vec은 자연어 처리(NLP) 분야에서 광범위하게 사용돼. 예를 들어:

- **문서 분류**: 단어를 벡터로 변환해 텍스트 분류 모델에 입력할 수 있어.
- **추천 시스템**: 사용자와 아이템 간의 유사성을 계산할 때 유용해.
- **질문 응답 시스템**: 유사한 질문을 찾거나 답변을 생성하는 데 활용돼.

맥락에서 단어 등장 (CBOW)
단어로 부터 맥장 등장(Skip-Gram)

Embedding : 인풋 내용

Context : 아웃풋 내용

### 정리

Word2Vec은 단어의 의미를 밀집 벡터로 표현하고, 단어 간의 관계를 잘 반영할 수 있는 강력한 도구야. 이로 인해 다양한 자연어 처리 작업에서 매우 유용하게 사용되고 있어!


dot product



**Dot Product**(내적 또는 점곱)은 두 벡터 간의 관계를 수치적으로 나타내는 연산이야. 주로 기하학적 의미와 함께 벡터 간의 유사성을 평가하는 데 사용돼. 

### 정의

두 벡터 \( \mathbf{A} = [a_1, a_2, \ldots, a_n] \)와 \( \mathbf{B} = [b_1, b_2, \ldots, b_n] \)의 내적은 다음과 같이 계산돼:

\[
\mathbf{A} \cdot \mathbf{B} = a_1 \times b_1 + a_2 \times b_2 + \ldots + a_n \times b_n
\]

### 예시

예를 들어, 두 벡터가 다음과 같다고 하자:

- \( \mathbf{A} = [2, 3, 4] \)
- \( \mathbf{B} = [1, 0, -1] \)

이 두 벡터의 내적은 다음과 같아:

\[
\mathbf{A} \cdot \mathbf{B} = (2 \times 1) + (3 \times 0) + (4 \times -1) = 2 + 0 - 4 = -2
\]

### 기하학적 의미

내적은 두 벡터 간의 각도와 관련이 있어. 두 벡터 \( \mathbf{A} \)와 \( \mathbf{B} \)의 내적은 다음과 같이 각도 \( \theta \)를 포함한 식으로 표현할 수 있어:

\[
\mathbf{A} \cdot \mathbf{B} = |\mathbf{A}| \cdot |\mathbf{B}| \cdot \cos(\theta)
\]

여기서 \( |\mathbf{A}| \)와 \( |\mathbf{B}| \)는 각 벡터의 크기야. 이 식을 통해 내적의 값이 양수인지, 음수인지, 혹은 0인지를 통해 두 벡터 간의 관계를 파악할 수 있어.

- **양수**: 두 벡터가 비슷한 방향을 가질 때 (각도가 0도 이하).
- **음수**: 두 벡터가 반대 방향을 가질 때 (각도가 90도 이상).
- **0**: 두 벡터가 수직일 때 (각도가 90도).

### 활용

- **유사도 측정**: 텍스트 분석이나 추천 시스템에서 두 벡터(예: 단어 임베딩, 사용자 프로필 등) 간의 유사성을 측정할 때 사용해.
- **물리학**: 힘과 이동의 관계를 계산할 때도 활용돼.

### 정리

Dot Product는 두 벡터 간의 관계를 수치적으로 나타내고, 기하학적으로도 의미가 있는 중요한 연산이야. 다양한 분야에서 널리 사용되고 있어!



backpropagation

**Backpropagation**(역전파)는 인공 신경망에서 가중치를 조정하기 위해 사용하는 알고리즘이야. 주로 딥러닝에서 모델을 훈련할 때 오류를 최소화하는 데 중요한 역할을 해. 이 과정은 다음과 같이 진행돼:

### 1. 순전파 (Forward Pass)

- 입력 데이터가 신경망을 통과하면서 각 레이어에서 계산이 이루어져. 이 과정에서 가중치와 활성화 함수를 사용해 출력값을 계산해.
- 예를 들어, 입력 \( X \)가 있을 때, 출력 \( Y \)는 \( Y = f(W \cdot X + b) \) 형태로 계산될 수 있어. 여기서 \( W \)는 가중치, \( b \)는 편향, \( f \)는 활성화 함수야.

### 2. 손실 계산

- 출력값 \( Y' \)와 실제 값 \( Y \) 간의 오차(손실)를 계산해. 손실 함수는 이 오차를 측정하는 데 사용되며, 일반적으로 Mean Squared Error (MSE) 또는 Cross-Entropy Loss가 사용돼.

### 3. 역전파 (Backpropagation)

- **오차 전파**: 손실을 기반으로 각 가중치가 얼마나 기여했는지를 계산해. 이 과정에서 연쇄 법칙(Chain Rule)을 사용해 미분을 통해 가중치의 기울기를 구해.
  
\[
\frac{\partial L}{\partial W} = \frac{\partial L}{\partial Y'} \cdot \frac{\partial Y'}{\partial W}
\]

여기서 \( L \)은 손실 함수, \( Y' \)는 예측된 값이야.

- **가중치 업데이트**: 계산된 기울기를 사용해 가중치를 업데이트해. 일반적으로 경사 하강법(Gradient Descent) 방법을 사용해 다음과 같이 업데이트해:

\[
W_{\text{new}} = W_{\text{old}} - \eta \cdot \frac{\partial L}{\partial W}
\]

여기서 \( \eta \)는 학습률(learning rate)로, 가중치를 얼마나 업데이트할지를 결정해.

### 4. 반복

- 이 과정을 여러 번 반복하면서 신경망이 학습해. 각 에포크(epoch)마다 모든 데이터에 대해 순전파와 역전파를 수행하고 가중치를 조정해.

### 활용

Backpropagation은 딥러닝에서 매우 중요한 알고리즘으로, 다음과 같은 분야에서 널리 사용돼:

- **이미지 인식**: CNN(합성곱 신경망)에서 이미지를 분류할 때.
- **자연어 처리**: RNN(순환 신경망)에서 문장을 이해하고 생성할 때.
- **추천 시스템**: 사용자와 아이템의 관계를 학습할 때.

### 정리

Backpropagation은 신경망의 가중치를 효율적으로 업데이트하는 방법으로, 신경망이 학습할 수 있도록 도와줘. 이 알고리즘 덕분에 깊은 신경망도 효과적으로 훈련할 수 있게 되었어!


graident = 기울기 / 민감도