분류를 하기 위한 첫 스텝은 Feature와 Label을 파악하는 것이라고 했습니다. Label은 우리가 원하는 분류 결과입니다

Label 결과에 영항을 주는 요소가 Feature입니다

X = Feature -> 광고성 단어 개수

y = Label = 1 -> 스팸 메일인 경우 
y = Label = 0 -> 스팸 메일이 아닌 경우 


 Training data set을 통해 classifier 모델을 훈련
 classfier 모델의 성능(performance)을 평가
 성능(performance)은 정밀도(accuracy),  정확성(Precision), 재현율(Recall) 등으로 측정



 df_filter =  encode_labels(X, X2, y, to_numpy=True)
# ncode_labels 함수
X_encoded, X2_encoded, y_encoded = encode_labels(X, X2, y, to_numpy=True)
#X_combined = list(zip(X_encoded, X2_encoded)) #튜플
X_combined = np.column_stack((X_encoded, X2_encoded)) #2D 배열



scaler.fit(X_train)
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)


X_train_scaled = scaler.fit_transform(X_train) 
X_test_scaled = scaler.transform(X_test)        

1번과 2번은 같지만 

X_train_scaled = scaler.fit_transform(X_train)  
X_test_scaled = scaler.fit_transform(X_test)

3번은 다르다



[[TN, FP],
 [FN, TP]]



 # 1. 마지막이 labels 인경우
#X = dataset.iloc[:, :-1].values  # 특성
#labels = dataset.iloc[:, -1].values  # 레이블

cluster 군집

유사도 함수 - > 색, 크기, 모양 

비지도는 지도 학습의 전처리 방법으로 쓰이기도 한다.


훈련 세트 정확도: 0.981
테스트 세트 정확도: 0.698




# 학습 분리
X, y_labels = split_features_and_target(df_encoded, 'survived')  # 특징과 타겟 분리
X_train, X_test, y_train, y_test = train_test_split(X, y_labels, test_size=0.3, random_state=109)

# 스케일화
#X_train_scaled, X_test_scaled = scale_data_SS(X_train, X_test)
X_train_scaled, X_test_scaled = scale_data_MMS(X_train, X_test)

from sklearn.linear_model import Lasso

# 모델 초기화 및 훈련
regression = Lasso(alpha=0.1)  # alpha는 λ에 해당
regression.fit(X, y_labels)

coefficients = regression.coef_
print(coefficients)

# 0이 아닌 계수를 가진 feature만 선택
important_features = [i for i in range(len(coefficients)) if coefficients[i] != 0]
print("선택된 feature의 인덱스:", important_features)
selected_column_names = [df.columns[i] for i in important_features]
print("선택된 feature의 열 이름:", selected_column_names,"전체 컬럼 이름" ,X.columns.tolist())

X_selected = X.iloc[:, important_features]
print("선택된 feature의 데이터:\n", selected_data)