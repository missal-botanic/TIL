딥러닝에서 **가중치**와 **바이어스**가 어떻게 적용되는지, 그리고 **비선형 활성화 함수**가 어떻게 그래프에 영향을 미치는지 시각적으로 확인할 수 있는 예제를 작성해보겠습니다.

이 코드는 세 가지 주요 부분을 다룹니다:
1. **선형 함수 (가중치와 바이어스를 적용한 직선)**: 가중치 `w`와 바이어스 `b`가 적용된 선형 방정식.
2. **활성화 함수의 적용 (비선형성 도입)**: ReLU, Sigmoid 활성화 함수의 변화를 시각화.
3. **다층 신경망**에서 각 층을 어떻게 처리하는지 보여주는 간단한 예시.

우리는 두 가지 경우를 시각화할 것입니다:
1. 선형 함수와 활성화 함수 없이.
2. ReLU와 Sigmoid 활성화 함수가 적용된 결과.

```python
import numpy as np
import matplotlib.pyplot as plt

# x 값을 생성 (예: -10부터 10까지 100개의 점)
x = np.linspace(-10, 10, 100)

# 가중치와 바이어스
w = 2  # 기울기 (가중치)
b = 1  # 절편 (바이어스)

# 선형 함수 (y = wx + b)
y_linear = w * x + b

# 활성화 함수: ReLU와 Sigmoid
def relu(z):
    return np.maximum(0, z)

def sigmoid(z):
    return 1 / (1 + np.exp(-z))

# ReLU와 Sigmoid의 적용
y_relu = relu(y_linear)
y_sigmoid = sigmoid(y_linear)

# 그래프 그리기
plt.figure(figsize=(14, 10))

# 1. 선형 함수
plt.subplot(2, 2, 1)
plt.plot(x, y_linear, label=f'Linear: y = {w}x + {b}', color='blue')
plt.title('Linear Function: y = wx + b')
plt.xlabel('x')
plt.ylabel('y')
plt.grid(True)
plt.axhline(0, color='black',linewidth=1)
plt.axvline(0, color='black',linewidth=1)
plt.legend()

# 2. ReLU 활성화 함수
plt.subplot(2, 2, 2)
plt.plot(x, y_relu, label='ReLU Activation', color='green')
plt.title('ReLU Activation: y = max(0, wx + b)')
plt.xlabel('x')
plt.ylabel('y')
plt.grid(True)
plt.axhline(0, color='black',linewidth=1)
plt.axvline(0, color='black',linewidth=1)
plt.legend()

# 3. Sigmoid 활성화 함수
plt.subplot(2, 2, 3)
plt.plot(x, y_sigmoid, label='Sigmoid Activation', color='red')
plt.title('Sigmoid Activation: y = 1 / (1 + exp(-wx - b))')
plt.xlabel('x')
plt.ylabel('y')
plt.grid(True)
plt.axhline(0, color='black',linewidth=1)
plt.axvline(0, color='black',linewidth=1)
plt.legend()

# 4. 다층 신경망 (두 층)
y_layer1 = w * x + b
y_layer2 = relu(y_layer1)
plt.subplot(2, 2, 4)
plt.plot(x, y_layer2, label='2-Layer Neural Network (ReLU)', color='purple')
plt.title('2-Layer Neural Network with ReLU')
plt.xlabel('x')
plt.ylabel('y')
plt.grid(True)
plt.axhline(0, color='black',linewidth=1)
plt.axvline(0, color='black',linewidth=1)
plt.legend()

# 그래프 레이아웃 조정
plt.tight_layout()
plt.show()
```

### 설명:
- **첫 번째 그래프**: `y = wx + b`인 **선형 함수**입니다. 가중치 \( w = 2 \)와 바이어스 \( b = 1 \)이 적용된 직선을 시각화합니다.
- **두 번째 그래프**: **ReLU** 활성화 함수가 적용된 직선입니다. ReLU는 입력이 음수일 때 0으로 만들고, 양수일 때는 그대로 유지합니다. 즉, 음수 값은 잘리고, 양수 값은 그대로 출력됩니다.
- **세 번째 그래프**: **Sigmoid** 활성화 함수가 적용된 직선입니다. Sigmoid는 출력 값을 0과 1 사이로 압축합니다. 이 함수는 주로 이진 분류 문제에서 사용됩니다.
- **네 번째 그래프**: **다층 신경망** 예시입니다. 첫 번째 층에서 선형 함수로 가중치와 바이어스를 적용한 후, ReLU 활성화 함수를 거쳐 출력됩니다. 이렇게 **다층 신경망**을 통해 비선형 함수를 모델링할 수 있습니다.

### 출력:
- **선형 함수**에서는 단순히 기울기와 절편에 의해 직선이 그려집니다.
- **ReLU**는 음수 값을 0으로 잘라내고, 양수 값은 그대로 유지합니다. 결과적으로 **"계단" 모양**의 그래프가 나옵니다.
- **Sigmoid**는 출력값을 0과 1 사이로 압축하여 **S자 형태**의 곡선을 생성합니다.
- **다층 신경망**에서는 첫 번째 층의 선형 결합 후, ReLU 활성화 함수가 적용되어 **복잡한 비선형 함수**가 됩니다.

이 시각화를 통해 **가중치**와 **바이어스**의 변화, 그리고 **비선형 활성화 함수**가 딥러닝 모델에서 어떻게 작동하는지 시각적으로 확인할 수 있습니다.