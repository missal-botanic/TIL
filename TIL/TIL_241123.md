

### 1. 모델 정의

먼저, PyTorch에서 신경망 모델을 정의하고 컴파일하는 방법을 알아보겠습니다.

```python
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from torch.utils.data import DataLoader, TensorDataset

# 모델 정의
class SimpleNN(nn.Module):
    def __init__(self, input_dim, num_classes):
        super(SimpleNN, self).__init__()
        self.layer1 = nn.Linear(input_dim, 64)
        self.layer2 = nn.Linear(64, 32)
        self.output_layer = nn.Linear(32, num_classes)
        self.softmax = nn.Softmax(dim=1)  # 소프트맥스 활성화 함수

    def forward(self, x):
        x = torch.relu(self.layer1(x))
        x = torch.relu(self.layer2(x))
        x = self.output_layer(x)
        x = self.softmax(x)  # 출력값을 확률로 변환
        return x
```

### 2. 데이터 준비

PyTorch에서 데이터를 다루려면 `DataLoader`를 사용해야 합니다. 여기에서는 간단한 예시로 데이터를 만들어 보겠습니다.

```python
# 예시 데이터 (입력 차원 4, 클래스 수 3)
X_train = np.random.rand(100, 4).astype(np.float32)  # 100개의 샘플, 각 샘플은 4차원
y_train = np.random.randint(0, 3, size=(100,))  # 3개의 클래스 (0, 1, 2)
y_train_one_hot = np.zeros((y_train.size, 3), dtype=np.float32)
y_train_one_hot[np.arange(y_train.size), y_train] = 1  # 원-핫 인코딩

# Tensor로 변환
X_train_tensor = torch.tensor(X_train)
y_train_tensor = torch.tensor(y_train_one_hot)

# DataLoader로 데이터 묶기
dataset = TensorDataset(X_train_tensor, y_train_tensor)
train_loader = DataLoader(dataset, batch_size=32, shuffle=True)
```

### 3. 모델 학습

모델을 학습시키기 위해 손실 함수와 옵티마이저를 설정하고, 여러 번의 에폭을 통해 학습을 진행합니다.

```python
# 모델 인스턴스화
model = SimpleNN(input_dim=4, num_classes=3)

# 손실 함수와 옵티마이저
criterion = nn.CrossEntropyLoss()  # CrossEntropyLoss는 기본적으로 소프트맥스를 내부에서 처리
optimizer = optim.Adam(model.parameters(), lr=0.001)

# 학습 루프
epochs = 10
for epoch in range(epochs):
    model.train()
    running_loss = 0.0
    for inputs, labels in train_loader:
        # 최적화 과정
        optimizer.zero_grad()
        outputs = model(inputs)  # 모델 예측 (softmax 확률 출력)
        loss = criterion(outputs, labels.argmax(dim=1))  # CrossEntropyLoss는 원핫 레이블을 처리
        loss.backward()
        optimizer.step()
        
        running_loss += loss.item()
    
    print(f"Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(train_loader):.4f}")
```

### 4. 예측하기

학습된 모델로 새로운 데이터에 대해 예측을 할 때, 각 클래스에 대한 확률을 출력하고, 가장 높은 확률을 가진 클래스를 선택할 수 있습니다.

```python
# 모델 평가 모드로 설정
model.eval()

# 예시 입력 데이터 (4차원 벡터)
new_data = np.array([[0.1, 0.2, 0.3, 0.4]], dtype=np.float32)
new_data_tensor = torch.tensor(new_data)

# 예측
with torch.no_grad():
    output = model(new_data_tensor)
    probabilities = output.numpy()  # 소프트맥스 결과는 확률로 변환됨
    print("Class probabilities:", probabilities)

    # 가장 높은 확률을 가진 클래스
    predicted_class = np.argmax(probabilities)
    print("Predicted class:", predicted_class)
```

### 5. 예시 출력

- `probabilities`는 각 클래스에 대한 확률을 담고 있습니다.
- `predicted_class`는 가장 높은 확률을 가진 클래스의 인덱스를 나타냅니다.

예를 들어, 출력은 다음과 같은 형태일 수 있습니다.

```python
Class probabilities: [[0.1 0.7 0.2]]
Predicted class: 1
```

여기서 첫 번째 클래스에 10%, 두 번째 클래스에 70%, 세 번째 클래스에 20% 확률이 할당되었으며, 모델은 두 번째 클래스(인덱스 1)로 예측한 것입니다.

### 요약

1. PyTorch에서 신경망을 정의할 때 `Softmax`를 사용하여 모델의 출력을 확률로 변환합니다.
2. 모델 학습 후 새로운 데이터를 예측하면, 출력이 각 클래스에 대한 확률을 포함합니다.
3. `argmax`를 사용하여 가장 높은 확률을 가진 클래스를 선택할 수 있습니다.

이러한 방식으로 PyTorch를 이용한 다중 클래스 분류 모델을 학습하고 예측 결과를 확률로 출력할 수 있습니다.