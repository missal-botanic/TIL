## 변수 하나
선형 회귀 # 1차원 (선)
다항 회귀 # 2차원 이상 (곡선)

## 변수 다수 (동시에 돌리고 선택)
L2 릿지 회귀 # 제곱값 합 - 수식적
L1 랏소 회귀 # 절대값 합 - 비슷한 스케일





### 선형 회귀 모델

```
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split as tts
from sklearn.linear_model import LinearRegression as LR
from sklearn.metrics import mean_squared_error, r2_score

X = np.array([[1, 1], [2, 2], [3, 3], [4, 4], [5, 5],[6,6]])
y = np.array([1, 2, 3, 4, 5, 6])

X_tr, X_te, y_tr, y_te = tts(X, y, test_size=0.2,random_state=42)

model = LR()
model.fit(X_tr, y_tr)

pre_v = model.predict(X_te)

print(mean_squared_error(y_te, pre_v))
print(r2_score(y_te, pre_v))
```

### 다항 회귀 (drgree 조절가능)



import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

X = np.array([[1], [2], [3], [4], [5], [6]])
y = np.array([1, 4, 9, 16, 25, 36])

poly = PolynomialFeatures(degree=2)
X_poly = poly.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(X_poly, y, test_size=0.2, random_state=42)

model = LinearRegression()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print(f'Mean Squared Error: {mse}')
print(f'R^2 Score: {r2}')

### 리지 라쏘 회귀 (규제 조절 가능 (alpha=0.1))

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt  # plt 임포트 추가
from sklearn.model_selection import train_test_split as tts
from sklearn.linear_model import Ridge, Lasso
from sklearn.metrics import mean_squared_error as MSE, r2_score as r2

X = np.array([[1, 1], [2, 2], [3, 3], [4, 4], [5, 5], [6, 6]])
y = np.array([1, 2, 3, 4, 5, 6])

X_tr, X_te, y_tr, y_te = tts(X, y, test_size=0.2, random_state=42)

#RG=Ridge(alpha=0.1)
#RG.fit(X_tr,y_tr)
#LS=Lasso(alpha=0.1)
#LS.fit(X_tr,y_tr)

RG_corr=[]
LS_corr=[]
i_num=0
for i in np.arange(0.01,5.0,0.1):
  RG=Ridge(alpha=i)
  RG.fit(X_tr,y_tr)
  LS=Lasso(alpha=i)
  LS.fit(X_tr,y_tr)
  RG_corr.append(np.corrcoef(RG.predict(X_te),y_te)[0][1])
  LS_corr.append(np.corrcoef(LS.predict(X_te),y_te)[0][1])
  i_num+=1
plt.plot(range(i_num),RG_corr,'b')
plt.plot(range(i_num),LS_corr,'r')


### SVM( StandardScaler 표준편차, MinMaxScale 선택 )


1. Linear Kernel
선형 분리를 사용합니다. 데이터가 선형적으로 분리 가능할 때 적합합니다.
```
SVC(kernel='linear')
```
2. Polynomial Kernel
다항식 커널을 사용하여 비선형 분리를 수행합니다.
```
SVC(kernel='poly', degree=3) # 여기서 degree는 다항식의 차수
``` 
3. Radial Basis Function (RBF) Kernel
가장 일반적으로 사용되는 커널로, 비선형 문제에 잘 작동합니다. 데이터의 밀집 지역을 기준으로 결정 경계를 형성합니다.
```
SVC(kernel='rbf')
```
4. Sigmoid Kernel
신경망의 활성화 함수와 유사한 방식으로 작동하는 커널입니다. 비선형성을 모델링할 수 있지만, 일반적으로 성능이 좋지 않습니다.
```
SVC(kernel='sigmoid')
```
5. Custom Kernel
사용자가 정의한 커널 함수를 사용할 수 있습니다. 이 경우, 커널 함수를 직접 구현해야 합니다.
```
SVC(kernel=custom_kernel_function)
```
추가 옵션
C: 규제 파라미터로, 오분류에 대한 패널티를 조정합니다. 기본값은 1.0입니다.
gamma: RBF 커널에 대한 파라미터로, 데이터 포인트의 영향 범위를 조정합니다. 낮은 값은 넓은 범위를, 높은 값은 좁은 범위를 의미합니다.
degree: Polynomial 커널의 차수를 설정합니다. 기본값은 3입니다.