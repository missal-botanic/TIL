ML_그래프그리기
=============

###  이미지를 부드럽게
```py
plt.imshow(wordcloud, interpolation='bilinear')
```

### 라인 그래프 그리기
```py
plt.scatter(input,output)
plt.plot(input,LR.coef_*input+LR.intercept,'r--')
```

### 2차원으로 군집 시각화
```py
plt.figure(figsize=(10, 8))
sns.scatterplot(x=data['Age'], y=data['Annual Income (k$)'], hue=data['Cluster'], palette='viridis')
plt.title('Clusters of customers (Age vs Annual Income)')
plt.show()
```

### 덴드로그램 생성
```py
plt.figure(figsize=(10, 7))
dendrogram = sch.dendrogram(sch.linkage(X_scaled, method='ward'))
plt.title('Dendrogram')
plt.xlabel('Customers')
plt.ylabel('Euclidean distances')
plt.show()
```

### 덴드로 그램으로 군집 나누기
```py
from scipy.cluster.hierarchy import linkage
input_linked=linkage(input,'ward')

from scipy.cluster.hierarchy import dendrogram
dendrogram(input_linked)
plt.show()

from scipy.cluster.hierarchy import fcluster
cluster_labels = fcluster(input_linked, 2,criterion='maxclust') 
cluster_labels

```

### 결정 트리 도식 보기
```py
plt.figure(figsize=(60,30))
plot_tree(DT,max_depth=3, filled=True, feature_names=iris.feature_names)
```

### 최적의 k 찾기 (엘보우 방법)
```py
inertia = []
K = range(1, 11)
for k in K:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(data_scaled)
    inertia.append(kmeans.inertia_)

# 엘보우 그래프 그리기
plt.figure(figsize=(10, 8))
plt.plot(K, inertia, 'bx-')
plt.xlabel('k')
plt.ylabel('Inertia')
plt.title('Elbow Method For Optimal k')
plt.show()

# k=5로 모델 생성 및 학습
kmeans = KMeans(n_clusters=5, random_state=42)
kmeans.fit(data_scaled)

# 군집 결과 할당
data['Cluster'] = kmeans.labels_
```

### 비교그래프
```py
RG_corr=[]
LS_corr=[]
i_num=0
for i in np.arange(0.01,5.0,0.1):
  RG=Ridge(alpha=i)
  RG.fit(X_tr,y_tr)
  LS=Lasso(alpha=i)
  LS.fit(X_tr,y_tr)
  RG_corr.append(np.corrcoef(RG.predict(X_te),y_te)[0][1])
  LS_corr.append(np.corrcoef(LS.predict(X_te),y_te)[0][1])
  i_num+=1
plt.plot(range(i_num),RG_corr,'b')
plt.plot(range(i_num),LS_corr,'r')

### best K value
acc_test=[]
acc_train=[]
for i in range(1, 100):
  knn_k=KNeighborsClassifier(n_neighbors=i)
  knn_k.fit(train_input,train_output)
  acc_test.append(knn_k.score(test_input,test_output))
  acc_train.append(knn_k.score(train_input,train_output))

plt.plot(range(1,100),acc_test,'r')
plt.plot(range(1,100),acc_train,'g')
```

### 계층적 군집화 모델 생성
```py
hc = AgglomerativeClustering(n_clusters=5, metric='euclidean', linkage='ward')

# 모델 학습 및 예측
y_hc = hc.fit_predict(X_scaled)

# 결과 시각화
plt.figure(figsize=(10, 7))
plt.scatter(X_scaled[y_hc == 0, 0], X_scaled[y_hc == 0, 1], s=100, c='red', label='Cluster 1')
plt.scatter(X_scaled[y_hc == 1, 0], X_scaled[y_hc == 1, 1], s=100, c='blue', label='Cluster 2')
plt.scatter(X_scaled[y_hc == 2, 0], X_scaled[y_hc == 2, 1], s=100, c='green', label='Cluster 3')
plt.scatter(X_scaled[y_hc == 3, 0], X_scaled[y_hc == 3, 1], s=100, c='cyan', label='Cluster 4')
plt.scatter(X_scaled[y_hc == 4, 0], X_scaled[y_hc == 4, 1], s=100, c='magenta', label='Cluster 5')
plt.title('Clusters of customers')
plt.xlabel('Age')
plt.ylabel('Annual Income (k$)')
plt.legend()
plt.show()
```

### 군집화 결과 시각화
```py
plt.figure(figsize=(10, 7))
sns.scatterplot(x='Annual Income (k$)', y='Spending Score (1-100)', hue='Cluster', data=df, palette='viridis')
plt.title('DBSCAN Clustering of Mall Customers')
plt.show()
```

### PCA 결과를 2차원 또는 3차원으로 시각화합니다.
```py
import matplotlib.pyplot as plt
import seaborn as sns

# 2차원 시각화
plt.figure(figsize=(10, 7))
sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=y, palette='viridis', legend=None)
plt.title('PCA of MNIST Dataset (2D)')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.show()
```

### 중요 특성 확인(랜덤 포레스트)
```py
import matplotlib.pyplot as plt
import seaborn as sns

# 특성 중요도 추출
feature_importances = rf_model.feature_importances_

# 특성 중요도를 데이터프레임으로 변환
feature_importances_df = pd.DataFrame({
    'Feature': X.columns,
    'Importance': feature_importances
})

# 중요도 순으로 정렬
feature_importances_df = feature_importances_df.sort_values(by='Importance', ascending=False)

# 특성 중요도 시각화
plt.figure(figsize=(10, 7))
sns.barplot(x='Importance', y='Feature', data=feature_importances_df)
plt.title('Feature Importances in Random Forest')
plt.show()
```

### 랜덤포레스트 시각화용
```py
from sklearn.tree import export_graphviz
RF_sample=RF.estimators_[0]

plt.figure(figsize=(60,30))
plot_tree(RF_sample, filled=True, feature_names=iris.feature_names)
```