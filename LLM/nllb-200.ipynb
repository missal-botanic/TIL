{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5db65ec7-fa51-4f92-b706-15e50c9d9c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "os.environ[\"HF_HOME\"] = r\"C:\\Users\\241011\\Documents\\models\"\n",
    "# NLLB-200 모델과 토크나이저 로드\n",
    "model_name = \"facebook/nllb-200-distilled-600M\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "56a0c005-dcc1-46b0-adaa-6c65eedb0bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(text, max_length=50):\n",
    "    paragraph = re.split(r'(?<=\\.)\\s*', text)\n",
    "    print(paragraph)\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    \n",
    "    for sentence in paragraph:\n",
    "        # 현재 문장이 추가될 경우 최대 길이를 초과하면, 현재까지 문장을 chunk로 저장\n",
    "        if len(current_chunk) + len(sentence) <= max_length:\n",
    "            current_chunk += sentence + \" \"  # 문장 추가 + 뒤에 공백 추가\n",
    "        else: # 크기가 초과 될 경우 \n",
    "            if current_chunk:  # 기존에 있는 chunk가 있으면 저장(처음에 통과 방지)\n",
    "                chunks.append(current_chunk.strip()) # 앞 뒤 공백 제거\n",
    "            current_chunk = sentence + \" \"  # 새로운 chunk 시작\n",
    "\n",
    "    if current_chunk: # 마지막 찌꺼기 문장이 100자 넘지 않을 경우 \n",
    "        chunks.append(current_chunk.strip())\n",
    "    return chunks\n",
    "\n",
    "# text = \"This is a sample text. It contains several sentences. We want to split it into smaller chunks. However, we must split it at periods, not in the middle of sentences.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6daa6b53-357f-484f-bd5a-c40fda757ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " This is a sample text. It contains several sentences. We want to split it into smaller chunks. However, we must split it at periods, not in the middle of sentences\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is a sample text.', 'It contains several sentences.', 'We want to split it into smaller chunks.', 'However, we must split it at periods, not in the middle of sentences.', '']\n",
      "['This is a sample text. It contains several sentences. We want to split it into smaller chunks.', 'However, we must split it at periods, not in the middle of sentences.']\n",
      "Translated text: 이것은 샘플 텍스트입니다. 몇 개의 문장을 포함합니다. 우리는 그것을 작은 조각으로 나누고 싶습니다.\n",
      "Translated text: 하지만 우리는 문장을 중간에서 나누는 것이 아니라 기간으로 나누어야 합니다.\n"
     ]
    }
   ],
   "source": [
    "# 번역할 문장\n",
    "sentence = input()\n",
    "\n",
    "chunks = split_text(text, max_length=100)\n",
    "print(chunks)\n",
    "\n",
    "for chunk in chunks: # 인덱스 값과 같이 출력 함수\n",
    "    inputs = tokenizer(chunk, return_tensors='pt') # NLLB-200에서 영어(Latin)와 한국어(Hangul) 코드 설정\n",
    "    generated_tokens = model.generate(inputs.input_ids, forced_bos_token_id = tokenizer.convert_tokens_to_ids('kor_Hang'), max_length=200) # 입력 문장에 대한 번역 수행 (영어 -> 한국어)\n",
    "    translated_text = tokenizer.decode(generated_tokens[0], skip_special_tokens=True) # 번역 결과 디코딩\n",
    "    print(f'Translated text: {translated_text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4afdfa89-7e29-4de4-9c30-6e84e42c01f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['안녕하세요', ' 반값습니다', '']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '안녕하세요. 반값습니다.'\n",
    "\n",
    "sentences = text.split(\".\")\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9373717-9697-403f-b10f-6a2586fc3e80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p310",
   "language": "python",
   "name": "p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
